{"cells":[{"cell_type":"code","source":["\nimport plotly.express as px\nimport pandas as pd\nimport numpy as np\nfrom scipy.stats import poisson\nimport datetime as dt\nfrom scipy import stats\npd.set_option('display.max_columns', None)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d2be9870-fd99-4ea7-a182-33086eae68e0"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["def L1_causality(df, result, level1,level2,level3,kpi1,start_date,end_date,period, baseline_window, field1, field2, field3, L1KPI, L1Num, L1Den, L2KPI1, L2Num1, L2Den1, L2KPI2, L2Num2, L2Den2, L2KPI3, L2Num3, L2Den3, L2KPI4, L2Num4, L2Den4, an_date, an_item1, an_item2, an_item3):\n    \n    #data is defined as the dataset used in the preceeding Anomaly Detection run\n    data = df\n\n    #KPI's are defined and data is cleaned\n    data = data.replace('#VALUE!', 0)\n    data = data.replace('#DIV/0!', 0)\n    data[L1KPI] = data[L1Num]/data[L1Den]\n    data[L2KPI1] = data[L2Num1]/data[L2Den1]\n    data[L2KPI2] = data[L2Num2]/data[L2Den2]\n    data[L2KPI3] = data[L2Num3]/data[L2Den3]\n    data[L2KPI4] = data[L2Num4]/data[L2Den4]\n    data = data.replace(np.inf, 0)\n    data = data.fillna(0)\n    data[f'{L2KPI4}'] = data[f'{L2KPI4}'].astype(float)\n    data[f'{L2KPI3}'] = data[f'{L2KPI3}'].astype(float)\n    data[f'{L2KPI2}'] = data[f'{L2KPI2}'].astype(float)\n    data['Par_Basis'] = data[L2KPI2] * data[L2KPI1] / data[L2KPI3]\n    \n    data = data.sort_values(by=['timestamp'])    \n    \n    timelist = list(range(0,baseline_window,1))\n    timeArray = np.array(timelist)\n    \n    group = list(data[field1].unique())\n    groups = []\n    for c in group:\n        locals()[c] = data.loc[data[field1] == c]\n        groups.append(locals()[c])\n        \n    #Iterates over each unique item in the field1 column and calculates trends for each KPI\n    for i in groups:\n      \n      i[f'Avg_{L2KPI1}'] = i[L2KPI1].rolling(window = baseline_window).mean()\n      i[f'Avg_{L2KPI2}'] = i[f'{L2KPI2}'].rolling(window = baseline_window).mean()\n      i[f'Avg_{L2KPI3}'] = i[f'{L2KPI3}'].rolling(window = baseline_window).mean()\n      i[f'Avg_{L2KPI4}'] = i[f'{L2KPI4}'].rolling(window = baseline_window).mean()\n      \n      #Trend is calculated for each KPI\n      Array = i[L1KPI].to_numpy()\n      Trend = []\n      for r in range(0, len(Array), 1):\n          if r>(baseline_window-2):\n              x = timeArray\n              y = Array[timeArray-(baseline_window-1)+r]\n              m, b = np.polyfit(x, y, 1)\n              Trend.append(m)\n          else:\n              Trend.append(0)\n      Trend = np.asarray(Trend)\n      i[f'{L1KPI}_Trend'] = Trend\n\n\n      Array = i[L2KPI1].to_numpy()\n      Trend = []\n      for r in range(0, len(Array), 1):\n          if r>(baseline_window-2):\n              x = timeArray\n              y = Array[timeArray-(baseline_window-1)+r]\n              m, b = np.polyfit(x, y, 1)\n              Trend.append(m)\n          else:\n              Trend.append(0)\n      Trend = np.asarray(Trend)\n      i[f'{L2KPI1}_Trend'] = Trend\n      i[f'{L2KPI2}_{L2KPI1}'] = i[f'Avg_{L2KPI2}'] / i[f'Avg_{L2KPI3}']\n      i[f'Par_{L2KPI1}'] = i[f'{L2KPI2}_{L2KPI1}'] * i[f'{L2KPI4}']\n      i[f'Par_{L2KPI4}'] = i[f'{L2KPI2}_{L2KPI1}'] / i[f'Avg_{L2KPI3}']\n\n\n      Array = i[L2KPI2].to_numpy()\n      Trend = []\n      for r in range(0, len(Array), 1):\n          if r>(baseline_window-2):\n              x = timeArray\n              y = Array[timeArray-(baseline_window-1)+r]\n              m, b = np.polyfit(x, y, 1)\n              Trend.append(m)\n          else:\n              Trend.append(0)\n\n      Trend = np.asarray(Trend)\n      i[f'{L2KPI2}_Trend'] = Trend\n      i[f'{L2KPI1}_{L2KPI3}']= i[f'Avg_{L2KPI1}'] / i[f'Avg_{L2KPI3}'] \n      i[f'Par_{L2KPI2}'] = i[f'{L2KPI1}_{L2KPI3}'] * i[f'{L2KPI4}']\n\n\n      Array = i[L2KPI3].to_numpy()\n      Trend = []\n      for r in range(0, len(Array), 1):\n          if r>(baseline_window-2):\n              x = timeArray\n              y = Array[timeArray-(baseline_window-1)+r]\n              m, b = np.polyfit(x, y, 1)\n              Trend.append(m)\n          else:\n              Trend.append(0)\n\n      \n      Trend = np.asarray(Trend)\n      i[f'{L2KPI3}_Trend'] = Trend\n      i[f'Frq_Ser_{L2KPI3}']= - (i[f'Avg_{L2KPI2}'] * i[f'Avg_{L2KPI1}'] * i[f'Avg_{L2KPI4}'])\n      i[f'Sq_{L2KPI3}'] = i[f'Avg_{L2KPI3}'] * i[f'Avg_{L2KPI3}'] \n      i[f'Par_{L2KPI3}'] = i[f'Frq_Ser_{L2KPI3}'] / i[f'Sq_{L2KPI3}']\n      i[f'Avg_{L1KPI}'] = i[f'{L1KPI}'].mean()\n\n      Array = i[f'{L2KPI4}'].to_numpy()\n      Trend = []\n      for r in range(0, len(Array), 1):\n          if r>(baseline_window-2):\n              x = timeArray\n              y = Array[timeArray-(baseline_window-1)+r]\n              m, b = np.polyfit(x, y, 1)\n              Trend.append(m)\n          else:\n              Trend.append(0)\n\n      Trend = np.asarray(Trend)\n      i[f'{L2KPI4}_Trend'] = Trend \n      i[f'Par_{L2KPI4}'] = i[f'{L2KPI2}_{L2KPI1}'] / i[f'Avg_{L2KPI3}']\n    \n    data = pd.concat(groups).fillna(0)\n    #Contribution metrics\n    \n    \n    data[f'{L2KPI3}_cont'] = data[f'{L2KPI3}_Trend'] * data[f'Par_{L2KPI3}']\n    data[f'{L2KPI1}_cont'] = data[f'{L2KPI1}_Trend'] * data[f'Par_{L2KPI1}']\n    data[f'{L2KPI2}_cont'] = data[f'{L2KPI2}_Trend'] * data[f'Par_{L2KPI2}']\n    data['Basis_cont'] = data[f'{L2KPI4}_Trend'] * data[f'Par_{L2KPI4}']\n    data[f'{L2KPI3}_%{L1KPI}'] = data[f'{L2KPI3}_cont'] / data[f'Avg_{L1KPI}']\n    data[f'{L2KPI1}_%{L1KPI}'] = data[f'{L2KPI1}_cont'] / data[f'Avg_{L1KPI}']\n    data[f'{L2KPI2}_%{L1KPI}'] = data[f'{L2KPI2}_cont'] / data[f'Avg_{L1KPI}']\n    data[f'Basis_%{L1KPI}'] = data['Basis_cont'] / data[f'Avg_{L1KPI}']\n    data[f'{L2KPI3}_atr'] = data[f'{L2KPI3}_cont'] / data[f'{L1KPI}_Trend']\n    data[f'{L2KPI1}_atr'] = data[f'{L2KPI1}_cont'] / data[f'{L1KPI}_Trend']\n    data[f'{L2KPI2}_atr'] = data[f'{L2KPI2}_cont'] / data[f'{L1KPI}_Trend']\n    data['Basis_atr'] = data['Basis_cont'] / data[f'{L1KPI}_Trend']\n    data[f'{L1KPI}_atr'] = data[f'{L2KPI3}_atr'] + data[f'{L2KPI1}_atr'] + data[f'{L2KPI2}_atr']\n\n    #Some final data cleaning and the calculation of the premium score by multiplying Trend and ULT_EARNED_PREMIUM\n    #Currently quoted out but also the ability to filter by date if the user wanted to analyze a single anomaly point\n\n    data[f'{L1KPI}_Trend'] = data[f'{L1KPI}_Trend'].astype(float)\n    data[f'{L2KPI3}'] = data[f'{L2KPI3}'].astype(float)\n    data.fillna(0)\n\n    #Filters the dataset to only show the outliers that are imported from the 'result' file and only\n    #In order to calculate accurate trend values I have to first analyze the full dataset and filter to show outliers after\n\n    results = data.loc[data['timestamp'] == an_date]\n    global kpiTrend\n    if an_item3 != '':\n      kpiTrend = results[f'{kpi1}_Trend'].loc[(results['timestamp'] == an_date) & (results['combo'] == (f'{an_item1},{an_item2},{an_item3}'))]\n      kpiTrend = kpiTrend.iloc[0]\n            \n    elif an_item2 !='':\n      kpiTrend = results[f'{kpi1}_Trend'].loc[(results['timestamp'] == an_date) & (results['combo'] == (f'{an_item1},{an_item2}'))]\n      kpiTrend = kpiTrend.iloc[0]\n      \n    else:\n      kpiTrend = results[f'{kpi1}_Trend'].loc[(results['timestamp'] == an_date) & (results['combo'] == an_item1)]\n      kpiTrend = kpiTrend.iloc[0]\n      \n    results = results[['timestamp', field1, f'{kpi1}_Trend', f'{L2KPI3}_cont',f'{L2KPI1}_cont', f'{L2KPI2}_cont', f'{L2KPI3}_%ALR', f'{L2KPI1}_%ALR', f'{L2KPI2}_%ALR', f'{L2KPI3}_atr', f'{L2KPI1}_atr', f'{L2KPI2}_atr', f'{L1KPI}_atr']]\n    #results.to_csv('Data/CausalityTest.csv', header = True, index = False)\n    display(results)\n    return(results)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"89be9a4d-7d49-4fb4-ad4c-b4b0e5624ddb"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["def L21_causality_1KPI(df, file_path, level1,level2,level3,kpi1,start_date,end_date,period, baseline_window, field1, field2, field3, an_item1, an_item2, an_item3, L1KPI, L1Num, L1Den, num_function_1, den_function_1, an_date, kpiTrend, l1_items, ranking_field, ranking_function, bins_cols, bins_size, date_field):\n  import glob\n  import os\n  from datetime import datetime, timedelta\n  #import pandasql as ps\n  from pyspark.sql import SQLContext\n  import pandas as pd\n  pd.options.mode.chained_assignment = None\n  \n  sqlContext = SQLContext(sc)\n  import time\n\n  start = time.time()\n  print(\"file read from dbfs started\")\n  \n\n  #file path\n  #path = '/dbfs/sg_claims_files/'\n  path = file_path\n  # check if string \"claims\" is part of the filepath - to identify claims/loss ratio database\n  if 'claims' in path:\n    #Read part train files from DBFS and concatenate into dataframe\n    all_files = glob.glob(os.path.join(path, \"SG_Claims_Training_0.ftr\"))     \n    df_from_each_file = (pd.read_feather(f) for f in all_files) #all columns\n    concatenated_df   = pd.concat(df_from_each_file, ignore_index=True)\n    # calcualte claims count and policy_cvog count from raw data (since these fields are required to calculate some KPI's it has been hardcoded. Assumption is that these fields will be already available in the data later)\n    concatenated_df['CLAIMS_COUNT']= np.where((concatenated_df['ULT_CLAIM_INCURRED'])>0, concatenated_df['CLAIMNUMBER'] + concatenated_df['COVERAGE_GROUP'] , '999')\n    concatenated_df['POL_CVG_COUNT']= np.where((concatenated_df['POLICY']).notna(), concatenated_df['POLICY'] + concatenated_df['COVERAGE_GROUP'] , '999')\n  #for billing and other database files which are single files in .csv format  \n  else:\n    #read file\n    concatenated_df = pd.read_csv(path)\n  \n  #Convert date_field to pandas datetime\n  concatenated_df[date_field] = pd.to_datetime(concatenated_df[date_field])\n  # Date Filter\n  concatenated_df = concatenated_df.loc[(concatenated_df[date_field] >= start_date) & (concatenated_df[date_field] <= end_date)]\n  \n  #Binning columns with user selected column names and bin sizes\n  for c in range(len(bins_cols)):\n    concatenated_df[bins_cols[c]+'_bins'] = pd.qcut(concatenated_df[bins_cols[c]], q=bins_size[c], precision = 0).astype(str)\n  \n  #Add month and year columns\n  concatenated_df['Accd_month'] = pd.to_datetime(concatenated_df[date_field]).dt.month_name()\n  concatenated_df['Accd_year'] = pd.to_datetime(concatenated_df[date_field]).dt.year\n  concatenated_df['Accd_quarter'] = pd.to_datetime(concatenated_df[date_field]).dt.quarter\n  \n  end = time.time()\n  print(\"file read from dbfs ended\",end - start)\n  \n  #Filter data on values in the anomaly result case as per the fields selected in Anomaly Detection. \n  if((field1 !=\"\") & (field2!=\"\") & (field3!=\"\")):\n       concatenated_df =  concatenated_df[( concatenated_df[field1] == an_item1) & ( concatenated_df[field2] == an_item2) & ( concatenated_df[field3] == an_item3)]\n      \n      #display(df)\n  \n  elif((field1 !=\"\")   & (field2==\"\") & (field3==\"\")): \n      concatenated_df = concatenated_df.loc[concatenated_df[field1] == an_item1]\n      \n      #display(concatenated_df)\n  \n  elif((field1 !=\"\")   & (field2 !=\"\" ) & (field3==\"\")):\n     concatenated_df =  concatenated_df[( concatenated_df[field1] == an_item1) & ( concatenated_df[field2] == an_item2)]\n    \n    #display(concatenated_df)\n  \n  #l1 data preprocessing------------------------------------------------------------------------\n  #Filter on level 1 field\n  \n  \n  #data = concatenated_df\n  #put aggregation columns in a list\n  lst1 = [level1]\n  #put columns for year and month in list\n  if(period == 'month'):\n      segment = ['Accd_year','Accd_month']\n      \n    #put columns for year and quarter in list\n  if(period == 'quarter'):\n      segment = ['Accd_year','Accd_quarter']\n      \n  segment.extend(lst1)\n  \n  if(level1!=''):\n\n\n    # calculate KPI values for KPIs selected by user\n      \n    data = concatenated_df.groupby(segment).agg({L1Num: num_function_1 , L1Den: den_function_1, ranking_field: ranking_function })\n    #data = concatenated_df.groupby(segment)\n    data.reset_index(inplace = True)\n    #display(data)\n    #if empty data set obtained after aggregation\n    if data.empty:\n      return('No records found for this selection!')\n    #Calculate values for KPI's for each row timeArray is used for the rolling average function. Creates an index with length defined by the baseline_window variable\n    data[L1KPI] = data[L1Num]/data[L1Den]\n    #Create timestamp column\n    if(period == 'month'):\n      data['Month Name-Year']=pd.to_datetime(data['Accd_month']+data['Accd_year'].astype(str),format='%B%Y')\n      data['timestamp'] = data['Month Name-Year'].dt.strftime('%Y-%m')\n\n    if(period == 'quarter'):\n      data['Accd_quarter']=data['Accd_quarter'].replace({1:'1', 2: '4', 3: '7', 4: '10'})\n      data['quart_year']=pd.to_datetime(data['Accd_year'].astype(str) +data['Accd_quarter'].astype(str),format='%Y%m')\n      data['timestamp'] = data['quart_year'].dt.strftime('%Y-%m')\n      \n\n    #Create a list of the dates within the rolling window used for causal analysis\n    data['timestamp'] = pd.to_datetime(data['timestamp'])\n    data =  data.sort_values(by='timestamp', ascending=True)\n    #Replace invalid values\n    data = data.replace('#DIV/0!', np.NaN)\n    data = data.replace('#VALUE!', np.NaN)\n    #Conversion to float\n    data[L1KPI] = data[L1KPI].astype(str).astype(float)\n    numeric_columns = data.select_dtypes(include=['number']).columns\n    from dateutil.relativedelta import relativedelta\n  \n    if(period == 'month'):\n      an_date = pd.to_datetime(an_date)\n      numlist = list(range(0,baseline_window,1))\n      timelist = []\n      for i in numlist:\n        past_date = an_date - relativedelta(months=baseline_window - 1 - i)\n        timelist.append( datetime(past_date.year, past_date.month, 1).strftime('%Y-%m-%d') )\n      print(timelist)    \n      timeArray = np.array(timelist)\n      \n    if(period == 'quarter'):\n      an_date = pd.to_datetime(an_date)\n      numlist = list(range(0,(baseline_window*3),3))\n      timelist = []\n      for i in numlist:\n        past_date = an_date - relativedelta(months=(baseline_window*3) - 3 - i)\n        past_date = past_date.strftime('%Y-%m-%d')\n        timelist.append(past_date)\n      print(timelist)  \n      timeArray = np.array(timelist)\n \n    #create a separate dataframe for each date in the rolling window\n    dgroups = []\n    for d in timelist:\n        locals()[d] = data.loc[data['timestamp'] == d]\n        dgroups.append(locals()[d])\n    data = pd.concat(dgroups).fillna(0)\n    \n    #timeArray is used for the rolling average function. Creates an index with length defined by the baseline_window variable\n    timelist = list(range(0,baseline_window,1))\n    timeArray = np.array(timelist)\n    len(timeArray)\n    data = data.fillna(0)\n\n    #Create a dataframe for each unique value in the level1 column\n    group = list(data[level1].unique())\n    groups = []\n    for c in group:\n        locals()[c] = data.loc[data[level1] == c]\n        groups.append(locals()[c])\n\n    for s in groups:\n        Array = s[L1KPI].to_numpy()\n        Trend = []\n        for i in range(0, len(Array), 1):\n            if i>(baseline_window-2):\n                x = timeArray\n                y = Array[timeArray-(baseline_window-1)+i]\n                m, b = np.polyfit(x, y, 1)\n                Trend.append(m)\n            else:\n                Trend.append(0)\n\n        Trend = np.asarray(Trend)\n        s[f'{L1KPI}_Trend'] = Trend\n\n        #Weighting basis\n        s[f'Avg_{L1KPI}'] = s[f'{L1KPI}'].rolling(window = baseline_window).mean()\n        s[f'Avg_{L1Num}'] = s[f'{L1Num}'].rolling(window = baseline_window).mean()\n        s[f'Avg_{L1Den}'] = s[f'{L1Den}'].rolling(window = baseline_window).mean()\n\n    data = pd.concat(groups).fillna(0)\n\n    date = list(data['timestamp'].unique())\n    dates = []\n    for c in date:\n        locals()[c] = data.loc[data['timestamp'] == c]\n        dates.append(locals()[c])\n\n    #Loops over each rolling window and calculates the weights for each occurance\n    for s in dates:\n        Array = s[f'Avg_{L1Den}'].to_numpy()\n        s[f'{L1KPI}_weight_basis'] = Array.sum()\n        \n    data = pd.concat(dates).fillna(0)\n\n    data[f'{L1KPI}_Weight'] = data[f'Avg_{L1Den}'] / data[f'{L1KPI}_weight_basis']\n\n    #Calculates aggregated weights to ensure it adds up to exactly 1\n    #data1 = data.loc[data['timestamp'] == an_date]\n    #data1[f'Agg_Weight'] = data1[f'{L1KPI}_Weight'].sum()\n\n    data[f'{L1KPI}_Slope_%ofAvg'] = data[f'{L1KPI}_Trend'] / data[f'Avg_{L1KPI}']\n    data[f'{L1KPI}_Trend_Decomp'] = data[f'{L1KPI}_Weight'] * data[f'{L1KPI}_Trend']\n\n    #Loops over each rolling window to calculate trend decomp and business mix\n    date = list(data['timestamp'].unique())\n    dates = []\n    for c in date:\n        locals()[c] = data.loc[data['timestamp'] == c]\n        dates.append(locals()[c])\n\n    for s in dates:\n        s[f'Agg_{L1KPI}_Trend_Decomp'] = s[f'{L1KPI}_Trend_Decomp'].sum()\n        s[f'{L1KPI}_Business_Mix'] = kpiTrend - s[f'{L1KPI}_Trend_Decomp'].sum()\n\n    data = pd.concat(dates).fillna(0)\n\n    data[f'{L1KPI}_Attribution'] = data[f'{L1KPI}_Trend_Decomp'] / kpiTrend\n    data[f'{L1KPI}_Impact_%{L1KPI}'] = data[f'{L1KPI}_Trend_Decomp'] / data[f'Avg_{L1KPI}']\n\n    date = list(data['timestamp'].unique())\n    dates = []\n    for c in date:\n        locals()[c] = data.loc[data['timestamp'] == c]\n        dates.append(locals()[c])\n\n    for s in dates:\n        s[f'{L1KPI}_Att_Business_Mix'] = s[f'{L1KPI}_Business_Mix'] / kpiTrend\n        s[f'{L1KPI}_Att_Agg'] = s[f'{L1KPI}_Attribution'].sum() + s[f'{L1KPI}_Att_Business_Mix']\n    data = pd.concat(dates).fillna(0)   \n    data[f'Agg_Trend_Decomp'] = data[f'{L1KPI}_Trend_Decomp'].sum()\n    result1 = data.loc[data['timestamp'] == an_date]\n    \n    \n    #if level 2 is defined, the trend value for the user specified KPI will be calculated to be used for the next drilldown\n    if (level2 != '') & (l1_items != ''):\n      global kpiTrend1\n      kpiTrend1 = result1[f'{kpi1}_Trend'].loc[result1[level1] == l1_items]\n      kpiTrend1 = kpiTrend1.iloc[0]\n      \n    result1 = result1[['timestamp', f'{level1}', f'{L1KPI}_Trend', f'{kpi1}_Trend_Decomp', f'Agg_{L1KPI}_Trend_Decomp', f'{kpi1}_Business_Mix', f'{kpi1}_Impact_%{kpi1}', f'{kpi1}_Attribution', f'{kpi1}_Att_Business_Mix', f'{kpi1}_Att_Agg']]\n    display(result1)\n    return(result1)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"9f981609-2e9d-49eb-a35a-a5daa55e1f3d"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["def L22_causality_1KPI(df, file_path, level1,level2,level3,kpi1,start_date,end_date,period, baseline_window, field1, field2, field3, an_item1, an_item2, an_item3, L1KPI, L1Num, L1Den, num_function_1, den_function_1, an_date, kpiTrend1, l1_items, l2_items, ranking_field, ranking_function, bins_cols, bins_size, date_field):\n  import glob\n  import os\n  from datetime import datetime, timedelta\n  #import pandasql as ps\n  from pyspark.sql import SQLContext\n  \n  sqlContext = SQLContext(sc)\n  import time\n  \n  #df = pd.read_csv('/dbfs/FileStore/CG/output_chubb/data.csv')\n  start = time.time()\n  print(\"file read from dbfs started\")\n  #Read part train files from DBFS and concatenate into dataframe  \n  #file path\n  #path = '/dbfs/sg_claims_files/'\n  path = file_path\n  # check if string \"claims\" is part of the filepath - to identify claims/loss ratio database\n  if 'claims' in path:\n    #Read part train files from DBFS and concatenate into dataframe\n    all_files = glob.glob(os.path.join(path, \"SG_Claims_Training_0.ftr\"))     \n    df_from_each_file = (pd.read_feather(f) for f in all_files) #all columns\n    concatenated_df   = pd.concat(df_from_each_file, ignore_index=True)\n    # calcualte claims count and policy_cvog count from raw data (since these fields are required to calculate some KPI's it has been hardcoded. Assumption is that these fields will be already available in the data later)\n    concatenated_df['CLAIMS_COUNT']= np.where((concatenated_df['ULT_CLAIM_INCURRED'])>0, concatenated_df['CLAIMNUMBER'] + concatenated_df['COVERAGE_GROUP'] , '999')\n    concatenated_df['POL_CVG_COUNT']= np.where((concatenated_df['POLICY']).notna(), concatenated_df['POLICY'] + concatenated_df['COVERAGE_GROUP'] , '999')\n  #for billing and other database files which are single files in .csv format  \n  else:\n    #read file\n    concatenated_df = pd.read_csv(path)\n  \n  #Convert date_field to pandas datetime\n  concatenated_df[date_field] = pd.to_datetime(concatenated_df[date_field])\n  # Date Filter\n  concatenated_df = concatenated_df.loc[(concatenated_df[date_field] >= start_date) & (concatenated_df[date_field] <= end_date)]\n  \n  #Binning columns with user selected column names and bin sizes\n  for c in range(len(bins_cols)):\n    concatenated_df[bins_cols[c]+'_bins'] = pd.qcut(concatenated_df[bins_cols[c]], q=bins_size[c], precision = 0).astype(str)\n  \n  #Add month and year columns\n  concatenated_df['Accd_month'] = pd.to_datetime(concatenated_df[date_field]).dt.month_name()\n  concatenated_df['Accd_year'] = pd.to_datetime(concatenated_df[date_field]).dt.year\n  concatenated_df['Accd_quarter'] = pd.to_datetime(concatenated_df[date_field]).dt.quarter\n  \n  #display(concatenated_df)\n  \n  end = time.time()\n  print(\"file read from dbfs ended\",end - start)\n  \n  #Filter data on values in the anomaly result case as per the fields selected in Anomaly Detection. \n  if((field1 !=\"\") & (field2!=\"\") & (field3!=\"\")):\n      concatenated_df =  concatenated_df[( concatenated_df[field1] == an_item1) & ( concatenated_df[field2] == an_item2) & ( concatenated_df[field3] == an_item3)]\n      \n      #display(df)\n  \n  elif((field1 !=\"\")   & (field2==\"\") & (field3==\"\")): \n      concatenated_df = concatenated_df.loc[concatenated_df[field1] == an_item1]\n      \n      #display(concatenated_df)\n  \n  elif((field1 !=\"\")   & (field2 !=\"\" ) & (field3==\"\")):\n     concatenated_df =  concatenated_df[( concatenated_df[field1] == an_item1) & ( concatenated_df[field2] == an_item2)]\n    \n    #display(concatenated_df)\n  \n  #l2 data preprocessing------------------------------------------------------------------------\n  #Filter on level 1 and level 2 field\n  \n  \n  data = concatenated_df\n  #put aggregation columns in a list\n  #put aggregation columns in a list\n  lst2 = [level1,level2]\n  #put columns for year and month in list\n  if(period == 'month'):\n      segment = ['Accd_year','Accd_month']\n      \n    #put columns for year and quarter in list\n  if(period == 'quarter'):\n      segment = ['Accd_year','Accd_quarter']\n      \n  segment.extend(lst2)\n  \n  \n  if((level1!='') & (level2!='')):\n    \n    # calculate KPI values for KPIs selected by user\n      \n    data = concatenated_df.groupby(segment).agg({L1Num: num_function_1 , L1Den: den_function_1, ranking_field: ranking_function })\n    #data = concatenated_df.groupby(segment)\n    data.reset_index(inplace = True)\n    #display(data)\n    #if empty data set obtained after aggregation\n    if data.empty:\n      return('No records found for this selection!')\n    #Calculate values for KPI's for each row timeArray is used for the rolling average function. Creates an index with length defined by the baseline_window variable\n    data[L1KPI] = data[L1Num]/data[L1Den]\n    \n    kpiTrend = 1\n    \n    #Create timestamp column\n    if(period == 'month'):\n      data['Month Name-Year']=pd.to_datetime(data['Accd_month']+data['Accd_year'].astype(str),format='%B%Y')\n      data['timestamp'] = data['Month Name-Year'].dt.strftime('%Y-%m')\n\n    if(period == 'quarter'):\n      data['Accd_quarter']=data['Accd_quarter'].replace({1:'1', 2: '4', 3: '7', 4: '10'})\n      data['quart_year']=pd.to_datetime(data['Accd_year'].astype(str) +data['Accd_quarter'].astype(str),format='%Y%m')\n      data['timestamp'] = data['quart_year'].dt.strftime('%Y-%m')\n      \n\n    #Create a list of the dates within the rolling window used for causal analysis\n    data['timestamp'] = pd.to_datetime(data['timestamp'])\n    data =  data.sort_values(by='timestamp', ascending=True)\n    #Replace invalid values\n    data = data.replace('#DIV/0!', np.NaN)\n    data = data.replace('#VALUE!', np.NaN)\n    #Conversion to float\n    data[L1KPI] = data[L1KPI].astype(str).astype(float)\n    numeric_columns = data.select_dtypes(include=['number']).columns\n    from dateutil.relativedelta import relativedelta\n  \n    if(period == 'month'):\n      an_date = pd.to_datetime(an_date)\n      numlist = list(range(0,baseline_window,1))\n      timelist = []\n      for i in numlist:\n        past_date = an_date - relativedelta(months=baseline_window - 1 - i)\n        #past_date = past_date.strftime('%Y-%m-%d')\n        timelist.append( datetime(past_date.year, past_date.month, 1).strftime('%Y-%m-%d') )\n        #timelist.append(past_date)\n      print(timelist)    \n      timeArray = np.array(timelist)\n      #dateArray = np.array(timelist)\n      #print(dateArray, timelist[0].type())\n      \n    if(period == 'quarter'):\n      an_date = pd.to_datetime(an_date)\n      numlist = list(range(0,(baseline_window*3),3))\n      timelist = []\n      for i in numlist:\n        past_date = an_date - relativedelta(months=(baseline_window*3) - 3 - i)\n        past_date = past_date.strftime('%Y-%m-%d')\n        timelist.append(past_date)\n      print(timelist)  \n      timeArray = np.array(timelist)\n    \n    dgroups = []\n    for d in timelist:\n        locals()[d] = data.loc[data['timestamp'] == d]\n        dgroups.append(locals()[d])\n    data = pd.concat(dgroups).fillna(0)\n    #display(data)\n    \n    #timeArray is used for the rolling average function. Creates an index with length defined by the \n    #baseline_window variable\n    timelist = list(range(0,baseline_window,1))\n    timeArray = np.array(timelist)\n    len(timeArray)\n    data = data.fillna(0)\n\n    group1 = list(data[level1].unique())\n    groups1 = []\n    for c in group1:\n        locals()[c] = data.loc[data[level1] == c]\n        groups1.append(locals()[c])\n        \n    group2 = list(data[level2].unique())\n    groups2 = []\n    for c in group2:\n        locals()[c] = data.loc[data[level2] == c]\n        groups2.append(locals()[c])\n        \n    for g in groups1:\n      for s in groups2:\n        Array = s[L1KPI].to_numpy()\n        Trend = []\n        for i in range(0, len(Array), 1):\n            if i>(baseline_window-2):\n                x = timeArray\n                y = Array[timeArray-(baseline_window-1)+i]\n                m, b = np.polyfit(x, y, 1)\n                Trend.append(m)\n            else:\n                Trend.append(0)\n\n        Trend = np.asarray(Trend)\n        s[f'{L1KPI}_Trend'] = Trend\n\n        #Weighting basis\n        s[f'Avg_{L1KPI}'] = s[f'{L1KPI}'].rolling(window = baseline_window).mean()\n        s[f'Avg_{L1Num}'] = s[f'{L1Num}'].rolling(window = baseline_window).mean()\n        s[f'Avg_{L1Den}'] = s[f'{L1Den}'].rolling(window = baseline_window).mean()\n\n    data = pd.concat(groups2).fillna(0)\n\n    date = list(data['timestamp'].unique())\n    dates = []\n    for c in date:\n        locals()[c] = data.loc[data['timestamp'] == c]\n        dates.append(locals()[c])\n\n    #Loops over each rolling window and calculates the weights for each occurance\n    for s in dates:\n        Array = s[f'Avg_{L1Den}'].to_numpy()\n        s[f'{L1KPI}_weight_basis'] = Array.sum()\n        \n    data = pd.concat(dates).fillna(0)\n\n    data[f'{L1KPI}_Weight'] = data[f'Avg_{L1Den}'] / data[f'{L1KPI}_weight_basis']\n\n    #Calculates aggregated weights to ensure it adds up to exactly 1\n    data1 = data.loc[data['timestamp'] == an_date]\n    data1[f'Agg_Weight'] = data1[f'{L1KPI}_Weight'].sum()\n\n    data[f'{L1KPI}_Slope_%ofAvg'] = data[f'{L1KPI}_Trend'] / data[f'Avg_{L1KPI}']\n    data[f'{L1KPI}_Trend_Decomp'] = data[f'{L1KPI}_Weight'] * data[f'{L1KPI}_Trend']\n\n    #Loops over each rolling window to calculate trend decomp and business mix\n    date = list(data['timestamp'].unique())\n    dates = []\n    for c in date:\n        locals()[c] = data.loc[data['timestamp'] == c]\n        dates.append(locals()[c])\n\n    for s in dates:\n        s[f'Agg_{L1KPI}_Trend_Decomp'] = s[f'{L1KPI}_Trend_Decomp'].sum()\n        s[f'{L1KPI}_Business_Mix'] = kpiTrend - s[f'{L1KPI}_Trend_Decomp'].sum()\n\n    data = pd.concat(dates).fillna(0)\n\n    data[f'{L1KPI}_Attribution'] = data[f'{L1KPI}_Trend_Decomp'] / kpiTrend\n    data[f'{L1KPI}_Impact_%{L1KPI}'] = data[f'{L1KPI}_Trend_Decomp'] / data[f'Avg_{L1KPI}']\n\n    date = list(data['timestamp'].unique())\n    dates = []\n    for c in date:\n        locals()[c] = data.loc[data['timestamp'] == c]\n        dates.append(locals()[c])\n\n    for s in dates:\n        s[f'{L1KPI}_Att_Business_Mix'] = s[f'{L1KPI}_Business_Mix'] / kpiTrend\n        s[f'{L1KPI}_Att_Agg'] = s[f'{L1KPI}_Attribution'].sum() + s[f'{L1KPI}_Att_Business_Mix']\n    data = pd.concat(dates).fillna(0)\n    #global kpiTrend1\n    #kpiTrend1 = data[f'{kpi1}_Trend']    \n    data[f'Agg_Trend_Decomp'] = data[f'{L1KPI}_Trend_Decomp'].sum()\n    \n    #create result file\n    result2 = data.loc[(data['timestamp'] == an_date) & (data[level1] == l1_items)]\n    \n    #if level 3 is defined, the trend value for the user specified KPI will be calculated to be used for the next drilldown\n    if (level3 != '') & (l2_items != ''):\n      global kpiTrend2\n      kpiTrend2 = result2[f'{kpi1}_Trend'].loc[result2[level2] == l2_items]\n      kpiTrend2 = kpiTrend2.iloc[0]\n    #result2 = result2.loc[result2[level1] == l1_items]\n    result2 = result2[['timestamp', f'{level2}',f'{level1}', f'{kpi1}_Trend', f'{kpi1}_Trend_Decomp',f'{kpi1}_Business_Mix', f'{kpi1}_Impact_%{kpi1}', f'{kpi1}_Attribution', f'{kpi1}_Att_Business_Mix', f'{kpi1}_Att_Agg']]\n    display(result2)\n    return(result2)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"4fe2465f-674a-4a87-a3de-1bad9196aeed"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["def L23_causality_1KPI(df, file_path, level1,level2,level3,kpi1,start_date,end_date,period, baseline_window, field1, field2, field3, an_item1, an_item2, an_item3, L1KPI, L1Num, L1Den, num_function_1, den_function_1, an_date, kpiTrend2, l1_items, l2_items, ranking_field, ranking_function, bins_cols, bins_size, date_field):\n  import glob\n  import os\n  from datetime import datetime, timedelta\n  #import pandasql as ps\n  from pyspark.sql import SQLContext\n  \n  sqlContext = SQLContext(sc)\n  import time\n  \n  #df = pd.read_csv('/dbfs/FileStore/CG/output_chubb/data.csv')\n  start = time.time()\n  print(\"file read from dbfs started\")\n  #Read part train files from DBFS and concatenate into dataframe  \n  #file path\n  #path = '/dbfs/sg_claims_files/'\n  path = file_path\n  # check if string \"claims\" is part of the filepath - to identify claims/loss ratio database\n  if 'claims' in path:\n    #Read part train files from DBFS and concatenate into dataframe\n    all_files = glob.glob(os.path.join(path, \"SG_Claims_Training_0.ftr\"))     \n    df_from_each_file = (pd.read_feather(f) for f in all_files) #all columns\n    concatenated_df   = pd.concat(df_from_each_file, ignore_index=True)\n    # calcualte claims count and policy_cvog count from raw data (since these fields are required to calculate some KPI's it has been hardcoded. Assumption is that these fields will be already available in the data later)\n    concatenated_df['CLAIMS_COUNT']= np.where((concatenated_df['ULT_CLAIM_INCURRED'])>0, concatenated_df['CLAIMNUMBER'] + concatenated_df['COVERAGE_GROUP'] , '999')\n    concatenated_df['POL_CVG_COUNT']= np.where((concatenated_df['POLICY']).notna(), concatenated_df['POLICY'] + concatenated_df['COVERAGE_GROUP'] , '999')\n  #for billing and other database files which are single files in .csv format  \n  else:\n    #read file\n    concatenated_df = pd.read_csv(path)\n  \n  #Convert date_field to pandas datetime\n  concatenated_df[date_field] = pd.to_datetime(concatenated_df[date_field])\n  # Date Filter\n  concatenated_df = concatenated_df.loc[(concatenated_df[date_field] >= start_date) & (concatenated_df[date_field] <= end_date)]\n  \n  #Binning columns with user selected column names and bin sizes\n  for c in range(len(bins_cols)):\n    concatenated_df[bins_cols[c]+'_bins'] = pd.qcut(concatenated_df[bins_cols[c]], q=bins_size[c], precision = 0).astype(str)\n  \n  #Add month and year columns\n  concatenated_df['Accd_month'] = pd.to_datetime(concatenated_df[date_field]).dt.month_name()\n  concatenated_df['Accd_year'] = pd.to_datetime(concatenated_df[date_field]).dt.year\n  concatenated_df['Accd_quarter'] = pd.to_datetime(concatenated_df[date_field]).dt.quarter\n  \n  #display(concatenated_df)\n  \n  end = time.time()\n  print(\"file read from dbfs ended\",end - start)\n  \n  #Filter data on values in the anomaly result case as per the fields selected in Anomaly Detection. \n  if((field1 !=\"\") & (field2!=\"\") & (field3!=\"\")):\n      concatenated_df =  concatenated_df[( concatenated_df[field1] == an_item1) & ( concatenated_df[field2] == an_item2) & ( concatenated_df[field3] == an_item3)]\n      \n      #display(df)\n  \n  elif((field1 !=\"\")   & (field2==\"\") & (field3==\"\")): \n      concatenated_df = concatenated_df.loc[concatenated_df[field1] == an_item1]\n      \n      #display(concatenated_df)\n  \n  elif((field1 !=\"\")   & (field2 !=\"\" ) & (field3==\"\")):\n     concatenated_df =  concatenated_df[( concatenated_df[field1] == an_item1) & ( concatenated_df[field2] == an_item2)]\n    \n    #display(concatenated_df)\n  \n  #l2 data preprocessing------------------------------------------------------------------------\n  #Filter on level 1 and level 2 field\n  \n  \n  data = concatenated_df\n  #put aggregation columns in a list\n  #put aggregation columns in a list\n  lst3 = [level1,level2,level3]\n  #put columns for year and month in list\n  if(period == 'month'):\n      segment = ['Accd_year','Accd_month']\n      \n    #put columns for year and quarter in list\n  if(period == 'quarter'):\n      segment = ['Accd_year','Accd_quarter']\n      \n  segment.extend(lst3)\n  \n  \n  if((level1!='') & (level2!='') & (level3!='')):\n    \n    #filter data on level1 values\n    concatenated_df = concatenated_df.loc[(concatenated_df[level1] == l1_items) & (concatenated_df[level2] == l2_items)]\n      \n    data = concatenated_df.groupby(segment).agg({L1Num: num_function_1 , L1Den: den_function_1, ranking_field: ranking_function})\n    #data = concatenated_df.groupby(segment)\n    data.reset_index(inplace = True)\n    #display(data)\n    #if empty data set obtained after aggregation\n    if data.empty:\n      return('No records found for this selection!')\n    #Calculate values for KPI's for each row timeArray is used for the rolling average function. Creates an index with length defined by the baseline_window variable\n    data[L1KPI] = data[L1Num]/data[L1Den]\n    \n    kpiTrend = 1\n    \n    #Create timestamp column\n    if(period == 'month'):\n      data['Month Name-Year']=pd.to_datetime(data['Accd_month']+data['Accd_year'].astype(str),format='%B%Y')\n      data['timestamp'] = data['Month Name-Year'].dt.strftime('%Y-%m')\n\n    if(period == 'quarter'):\n      data['Accd_quarter']=data['Accd_quarter'].replace({1:'1', 2: '4', 3: '7', 4: '10'})\n      data['quart_year']=pd.to_datetime(data['Accd_year'].astype(str) +data['Accd_quarter'].astype(str),format='%Y%m')\n      data['timestamp'] = data['quart_year'].dt.strftime('%Y-%m')\n      \n\n    #Create a list of the dates within the rolling window used for causal analysis\n    data['timestamp'] = pd.to_datetime(data['timestamp'])\n    data =  data.sort_values(by='timestamp', ascending=True)\n    #Replace invalid values\n    data = data.replace('#DIV/0!', np.NaN)\n    data = data.replace('#VALUE!', np.NaN)\n    #Conversion to float\n    data[L1KPI] = data[L1KPI].astype(str).astype(float)\n    numeric_columns = data.select_dtypes(include=['number']).columns\n  \n    if(period == 'month'):\n      from dateutil.relativedelta import relativedelta\n      an_date = pd.to_datetime(an_date)\n      numlist = list(range(0,baseline_window,1))\n      timelist = []\n      for i in numlist:\n        past_date = an_date - relativedelta(months=baseline_window - 1 - i)\n        #past_date = past_date.strftime('%Y-%m-%d')\n        timelist.append( datetime(past_date.year, past_date.month, 1).strftime('%Y-%m-%d') )\n        #timelist.append(past_date)\n      print(timelist)    \n      timeArray = np.array(timelist)\n      #dateArray = np.array(timelist)\n      #print(dateArray, timelist[0].type())\n      \n    if(period == 'quarter'):\n      from dateutil.relativedelta import relativedelta\n      an_date = pd.to_datetime(an_date)\n      numlist = list(range(0,(baseline_window*3),3))\n      timelist = []\n      for i in numlist:\n        past_date = an_date - relativedelta(months=(baseline_window*3) - 3 - i)\n        past_date = past_date.strftime('%Y-%m-%d')\n        timelist.append(past_date)\n      print(timelist)  \n      timeArray = np.array(timelist)\n    \n    dgroups = []\n    for d in timelist:\n        locals()[d] = data.loc[data['timestamp'] == d]\n        dgroups.append(locals()[d])\n    data = pd.concat(dgroups).fillna(0)\n    #display(data)\n    \n    #timeArray is used for the rolling average function. Creates an index with length defined by the \n    #baseline_window variable\n    timelist = list(range(0,baseline_window,1))\n    timeArray = np.array(timelist)\n    len(timeArray)\n    data = data.fillna(0)\n\n    group1 = list(data[level1].unique())\n    groups1 = []\n    for c in group1:\n        locals()[c] = data.loc[data[level1] == c]\n        groups1.append(locals()[c])\n        \n    group2 = list(data[level2].unique())\n    groups2 = []\n    for c in group2:\n        locals()[c] = data.loc[data[level2] == c]\n        groups2.append(locals()[c])\n        \n    group3 = list(data[level3].unique())\n    groups3 = []\n    for c in group3:\n        locals()[c] = data.loc[data[level3] == c]\n        groups3.append(locals()[c])\n    \n    for b in group1:    \n      for g in groups2:\n        for s in groups3:\n          Array = s[L1KPI].to_numpy()\n          Trend = []\n          for i in range(0, len(Array), 1):\n              if i>(baseline_window-2):\n                  x = timeArray\n                  y = Array[timeArray-(baseline_window-1)+i]\n                  m, b = np.polyfit(x, y, 1)\n                  Trend.append(m)\n              else:\n                  Trend.append(0)\n\n          Trend = np.asarray(Trend)\n          s[f'{L1KPI}_Trend'] = Trend\n\n          #Weighting basis\n          s[f'Avg_{L1KPI}'] = s[f'{L1KPI}'].rolling(window = baseline_window).mean()\n          s[f'Avg_{L1Num}'] = s[f'{L1Num}'].rolling(window = baseline_window).mean()\n          s[f'Avg_{L1Den}'] = s[f'{L1Den}'].rolling(window = baseline_window).mean()\n\n    data = pd.concat(groups3).fillna(0)\n\n    date = list(data['timestamp'].unique())\n    dates = []\n    for c in date:\n        locals()[c] = data.loc[data['timestamp'] == c]\n        dates.append(locals()[c])\n\n    #Loops over each rolling window and calculates the weights for each occurance\n    for s in dates:\n        Array = s[f'Avg_{L1Den}'].to_numpy()\n        s[f'{L1KPI}_weight_basis'] = Array.sum()\n        \n    data = pd.concat(dates).fillna(0)\n\n    data[f'{L1KPI}_Weight'] = data[f'Avg_{L1Den}'] / data[f'{L1KPI}_weight_basis']\n\n    #Calculates aggregated weights to ensure it adds up to exactly 1\n    data1 = data.loc[data['timestamp'] == an_date]\n    data1[f'Agg_Weight'] = data1[f'{L1KPI}_Weight'].sum()\n\n    data[f'{L1KPI}_Slope_%ofAvg'] = data[f'{L1KPI}_Trend'] / data[f'Avg_{L1KPI}']\n    data[f'{L1KPI}_Trend_Decomp'] = data[f'{L1KPI}_Weight'] * data[f'{L1KPI}_Trend']\n\n    #Loops over each rolling window to calculate trend decomp and business mix\n    date = list(data['timestamp'].unique())\n    dates = []\n    for c in date:\n        locals()[c] = data.loc[data['timestamp'] == c]\n        dates.append(locals()[c])\n\n    for s in dates:\n        s[f'Agg_{L1KPI}_Trend_Decomp'] = s[f'{L1KPI}_Trend_Decomp'].sum()\n        s[f'{L1KPI}_Business_Mix'] = kpiTrend - s[f'{L1KPI}_Trend_Decomp'].sum()\n\n    data = pd.concat(dates).fillna(0)\n\n    data[f'{L1KPI}_Attribution'] = data[f'{L1KPI}_Trend_Decomp'] / kpiTrend\n    data[f'{L1KPI}_Impact_%{L1KPI}'] = data[f'{L1KPI}_Trend_Decomp'] / data[f'Avg_{L1KPI}']\n\n    date = list(data['timestamp'].unique())\n    dates = []\n    for c in date:\n        locals()[c] = data.loc[data['timestamp'] == c]\n        dates.append(locals()[c])\n\n    for s in dates:\n        s[f'{L1KPI}_Att_Business_Mix'] = s[f'{L1KPI}_Business_Mix'] / kpiTrend\n        s[f'{L1KPI}_Att_Agg'] = s[f'{L1KPI}_Attribution'].sum() + s[f'{L1KPI}_Att_Business_Mix']\n    data = pd.concat(dates).fillna(0)\n    #global kpiTrend1\n    #kpiTrend1 = data[f'{kpi1}_Trend']    \n    data[f'Agg_Trend_Decomp'] = data[f'{L1KPI}_Trend_Decomp'].sum()\n    \n    #create result file\n    result3 = data.loc[(data['timestamp'] == an_date) & (data[level1] == l1_items) & (data[level2] == l2_items)]\n    result3 = result3[['timestamp', f'{level3}', f'{kpi1}_Trend', f'{kpi1}_Trend_Decomp',f'{kpi1}_Business_Mix', f'{kpi1}_Impact_%{kpi1}', f'{kpi1}_Attribution', f'{kpi1}_Att_Business_Mix', f'{kpi1}_Att_Agg']]\n    display(result3)\n    return(result3)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"42cd62fb-2295-494c-9bf1-daa7c15eafec"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["def L21_causality(df, file_path, level1,level2,level3,kpi1,start_date,end_date,period, baseline_window, field1, field2, field3, an_item1, an_item2, an_item3, L1KPI, L1Num, L1Den, num_function_1, den_function_1, L2KPI1, L2Num1, L2Den1, sub_num_function_1, sub_den_function_1, L2KPI2, L2Num2, L2Den2, sub_num_function_2, sub_den_function_2, L2KPI3, L2Num3, L2Den3, sub_num_function_3, sub_den_function_3, L2KPI4, L2Num4, L2Den4, sub_num_function_4, sub_den_function_4, an_date, kpiTrend, l1_items, ranking_field, ranking_function, bins_cols, bins_size, date_field):\n  import glob\n  import os\n  from datetime import datetime, timedelta\n  #import pandasql as ps\n  from pyspark.sql import SQLContext\n  import pandas as pd\n  pd.options.mode.chained_assignment = None\n  \n  sqlContext = SQLContext(sc)\n  import time\n\n  start = time.time()\n  print(\"file read from dbfs started\")\n  \n\n  #file path\n  #path = '/dbfs/sg_claims_files/'\n  path = file_path\n  # check if string \"claims\" is part of the filepath - to identify claims/loss ratio database\n  if 'claims' in path:\n    #Read part train files from DBFS and concatenate into dataframe\n    all_files = glob.glob(os.path.join(path, \"SG_Claims_Training_0.ftr\"))     \n    df_from_each_file = (pd.read_feather(f) for f in all_files) #all columns\n    concatenated_df   = pd.concat(df_from_each_file, ignore_index=True)\n    # calcualte claims count and policy_cvog count from raw data (since these fields are required to calculate some KPI's it has been hardcoded. Assumption is that these fields will be already available in the data later)\n    concatenated_df['CLAIMS_COUNT']= np.where((concatenated_df['ULT_CLAIM_INCURRED'])>0, concatenated_df['CLAIMNUMBER'] + concatenated_df['COVERAGE_GROUP'] , '999')\n    concatenated_df['POL_CVG_COUNT']= np.where((concatenated_df['POLICY']).notna(), concatenated_df['POLICY'] + concatenated_df['COVERAGE_GROUP'] , '999')\n  #for billing and other database files which are single files in .csv format  \n  else:\n    #read file\n    concatenated_df = pd.read_csv(path)\n  \n  #Convert date_field to pandas datetime\n  concatenated_df[date_field] = pd.to_datetime(concatenated_df[date_field])\n  # Date Filter\n  concatenated_df = concatenated_df.loc[(concatenated_df[date_field] >= start_date) & (concatenated_df[date_field] <= end_date)]\n  \n  #Binning columns with user selected column names and bin sizes\n  for c in range(len(bins_cols)):\n    concatenated_df[bins_cols[c]+'_bins'] = pd.qcut(concatenated_df[bins_cols[c]], q=bins_size[c], precision = 0).astype(str)\n  \n  #Add month and year columns\n  concatenated_df['Accd_month'] = pd.to_datetime(concatenated_df[date_field]).dt.month_name()\n  concatenated_df['Accd_year'] = pd.to_datetime(concatenated_df[date_field]).dt.year\n  concatenated_df['Accd_quarter'] = pd.to_datetime(concatenated_df[date_field]).dt.quarter\n  \n  end = time.time()\n  print(\"file read from dbfs ended\",end - start)\n  \n  #Filter data on values in the anomaly result case as per the fields selected in Anomaly Detection. \n  if((field1 !=\"\") & (field2!=\"\") & (field3!=\"\")):\n       concatenated_df =  concatenated_df[( concatenated_df[field1] == an_item1) & ( concatenated_df[field2] == an_item2) & ( concatenated_df[field3] == an_item3)]\n      \n      #display(df)\n  \n  elif((field1 !=\"\")   & (field2==\"\") & (field3==\"\")): \n      concatenated_df = concatenated_df.loc[concatenated_df[field1] == an_item1]\n      \n      #display(concatenated_df)\n  \n  elif((field1 !=\"\")   & (field2 !=\"\" ) & (field3==\"\")):\n     concatenated_df =  concatenated_df[( concatenated_df[field1] == an_item1) & ( concatenated_df[field2] == an_item2)]\n    \n    #display(concatenated_df)\n  \n  #l1 data preprocessing------------------------------------------------------------------------\n  #Filter on level 1 field\n  \n  #put aggregation columns in a list\n  lst1 = [level1]\n  #put columns for year and month in list\n  if(period == 'month'):\n      segment = ['Accd_year','Accd_month']\n      \n    #put columns for year and quarter in list\n  if(period == 'quarter'):\n      segment = ['Accd_year','Accd_quarter']\n      \n  segment.extend(lst1)\n  \n  if(level1!=''):\n\n\n    # calculate KPI values for KPIs selected by user\n    if((L2KPI1 !=\"\") & (L2KPI2!=\"\") & (L2KPI3!=\"\") & (L2KPI4!=\"\")):\n      \n      data = concatenated_df.groupby(segment).agg({L1Num: num_function_1 , L1Den: den_function_1, L2Num1:sub_num_function_1, L2Den1: sub_den_function_1, L2Num2:sub_num_function_2, L2Den2:sub_den_function_2, L2Num3:sub_num_function_3, L2Den3:sub_den_function_3, L2Num4:sub_num_function_4, L2Den4:sub_den_function_4, ranking_field: ranking_function})\n      #data = concatenated_df.groupby(segment)\n      data.reset_index(inplace = True)\n      #if empty data set obtained after aggregation\n      if data.empty:\n        return('No records found for this selection!')\n    #Calculate values for KPI's for each row timeArray is used for the rolling average function. Creates an index with length defined by the baseline_window variable\n    data[L1KPI] = data[L1Num]/data[L1Den]\n    data[L2KPI1] = data[L2Num1]/data[L2Den1]\n    data[L2KPI2] = data[L2Num2]/data[L2Den2]\n    data[L2KPI3] = data[L2Num3]/data[L2Den3]\n    data[f'{L2KPI3}'] = data[f'{L2KPI3}'].astype(float)\n    data[f'{L2Den3}'] = data[f'{L2Num3}']/data[f'{L2KPI3}']\n    \n    #Create timestamp column\n    if(period == 'month'):\n      data['Month Name-Year']=pd.to_datetime(data['Accd_month']+data['Accd_year'].astype(str),format='%B%Y')\n      data['timestamp'] = data['Month Name-Year'].dt.strftime('%Y-%m')\n\n    if(period == 'quarter'):\n      data['Accd_quarter']=data['Accd_quarter'].replace({1:'1', 2: '4', 3: '7', 4: '10'})\n      data['quart_year']=pd.to_datetime(data['Accd_year'].astype(str) +data['Accd_quarter'].astype(str),format='%Y%m')\n      data['timestamp'] = data['quart_year'].dt.strftime('%Y-%m')\n      \n\n\n    data['timestamp'] = pd.to_datetime(data['timestamp'])\n    data =  data.sort_values(by='timestamp', ascending=True)\n    #Replace invalid values\n    data = data.replace('#DIV/0!', np.NaN)\n    data = data.replace('#VALUE!', np.NaN)\n    #Conversion to float\n    data[L1KPI] = data[L1KPI].astype(str).astype(float)\n    numeric_columns = data.select_dtypes(include=['number']).columns\n    from dateutil.relativedelta import relativedelta\n\n    #Create a list of the dates within the rolling window used for causal analysis\n    if(period == 'month'):\n      an_date = pd.to_datetime(an_date)\n      numlist = list(range(0,baseline_window,1))\n      timelist = []\n      for i in numlist:\n        past_date = an_date - relativedelta(months=baseline_window - 1 - i)\n        timelist.append( datetime(past_date.year, past_date.month, 1).strftime('%Y-%m-%d') )\n      print(timelist)    \n      timeArray = np.array(timelist)\n      \n    if(period == 'quarter'):\n      an_date = pd.to_datetime(an_date)\n      numlist = list(range(0,(baseline_window*3),3))\n      timelist = []\n      for i in numlist:\n        past_date = an_date - relativedelta(months=(baseline_window*3) - 3 - i)\n        past_date = past_date.strftime('%Y-%m-%d')\n        timelist.append(past_date)\n      print(timelist)  \n      timeArray = np.array(timelist)\n    \n    dgroups = []\n    for d in timelist:\n        locals()[d] = data.loc[data['timestamp'] == d]\n        dgroups.append(locals()[d])\n    data = pd.concat(dgroups).fillna(0)\n    #display(data)\n    \n    #timeArray is used for the rolling average function. Creates an index with length defined by the baseline_window variable\n    timelist = list(range(0,baseline_window,1))\n    timeArray = np.array(timelist)\n    len(timeArray)\n    data = data.fillna(0)\n\n    group = list(data[level1].unique())\n    groups = []\n    for c in group:\n        locals()[c] = data.loc[data[level1] == c]\n        groups.append(locals()[c])\n\n    for s in groups:\n        Array = s[L1KPI].to_numpy()\n        Trend = []\n        for i in range(0, len(Array), 1):\n            if i>(baseline_window-2):\n                x = timeArray\n                y = Array[timeArray-(baseline_window-1)+i]\n                m, b = np.polyfit(x, y, 1)\n                Trend.append(m)\n            else:\n                Trend.append(0)\n\n        Trend = np.asarray(Trend)\n        s[f'{L1KPI}_Trend'] = Trend\n\n        #Weighting basis\n        s[f'Avg_{L1KPI}'] = s[f'{L1KPI}'].rolling(window = baseline_window).mean()\n        s[f'Avg_{L2KPI1}'] = s[f'{L2KPI1}'].rolling(window = baseline_window).mean()\n        s[f'Avg_{L2KPI2}'] = s[f'{L2KPI2}'].rolling(window = baseline_window).mean()\n        s[f'Avg_{L2KPI3}'] = s[f'{L2KPI3}'].rolling(window = baseline_window).mean()\n        s[f'Avg_{L2Num2}'] = s[f'{L2Num2}'].rolling(window = baseline_window).mean()\n        s[f'Avg_{L2Den2}'] = s[f'{L2Den2}'].rolling(window = baseline_window).mean()\n        s[f'Avg_{L2Den3}'] = s[f'{L2Den3}'].rolling(window = baseline_window).mean()\n\n        #Calculate slope for {L2KPI1} metric\n        Array = s[f'{L2KPI1}'].to_numpy()\n        Trend = []\n        for i in range(0, len(Array), 1):\n            if i>(baseline_window-2):\n                x = timeArray\n                y = Array[timeArray-(baseline_window-1)+i]\n                m, b = np.polyfit(x, y, 1)\n                Trend.append(m)\n            else:\n                Trend.append(0)\n        Trend = np.asarray(Trend)\n        s[f'{L2KPI1}_Trend'] = Trend\n\n        #Calculate slope for {L2KPI2} metric\n        Array = s[f'{L2KPI2}'].to_numpy()\n        Trend = []\n        for i in range(0, len(Array), 1):\n            if i>(baseline_window-2):\n                x = timeArray\n                y = Array[timeArray-(baseline_window-1)+i]\n                m, b = np.polyfit(x, y, 1)\n                Trend.append(m)\n            else:\n                Trend.append(0)\n\n        Trend = np.asarray(Trend)\n        s[f'{L2KPI2}_Trend'] = Trend\n\n        #Calculate slope for {L2KPI3} metric\n        Array = s[f'{L2KPI3}'].to_numpy()\n        Trend = []\n        for i in range(0, len(Array), 1):\n            if i>(baseline_window-2):\n                x = timeArray\n                y = Array[timeArray-(baseline_window-1)+i]\n                m, b = np.polyfit(x, y, 1)\n                Trend.append(m)\n            else:\n                Trend.append(0)\n\n        Trend = np.asarray(Trend)\n        s[f'{L2KPI3}_Trend'] = Trend\n\n    data = pd.concat(groups).fillna(0)\n\n    date = list(data['timestamp'].unique())\n    dates = []\n    for c in date:\n        locals()[c] = data.loc[data['timestamp'] == c]\n        dates.append(locals()[c])\n\n    #Loops over each rolling window and calculates the weights for each occurance\n    for s in dates:\n        Array = s[f'Avg_{L2KPI3}'].to_numpy()\n        s[f'{L1KPI}_weight_basis'] = Array.sum()\n\n        Array = s[f'Avg_{L2Num2}'].to_numpy()\n        s[f'{L2KPI1}_weight_basis'] = Array.sum()\n\n        Array = s[f'Avg_{L2Den2}'].to_numpy()\n        s[f'{L2KPI2}_weight_basis'] = Array.sum()\n\n        Array = s[f'Avg_{L2Den3}'].to_numpy()\n        s[f'{L2KPI3}_weight_basis'] = Array.sum()\n    data = pd.concat(dates).fillna(0)\n\n    data[f'{L1KPI}_Weight'] = data[f'Avg_{L2KPI3}'] / data[f'{L1KPI}_weight_basis']\n    data[f'{L2KPI1}_Weight'] = data[f'Avg_{L2Num2}'] / data[f'{L2KPI1}_weight_basis']\n    data[f'{L2KPI2}_Weight'] = data[f'Avg_{L2Den2}'] / data[f'{L2KPI2}_weight_basis']\n    data[f'{L2KPI3}_Weight'] = data[f'Avg_{L2Den3}'] / data[f'{L2KPI3}_weight_basis']\n\n    #Calculates aggregated weights to ensure it adds up to exactly 1\n    data1 = data.loc[data['timestamp'] == an_date]\n    data1[f'Agg_Weight'] = data1[f'{L1KPI}_Weight'].sum()\n\n    data[f'{L1KPI}_Slope_%ofAvg'] = data[f'{L1KPI}_Trend'] / data[f'Avg_{L1KPI}']\n    data[f'{L2KPI1}_Slope_%ofAvg'] = data[f'{L2KPI1}_Trend'] / data[f'Avg_{L2KPI1}']\n    data[f'{L2KPI2}_Slope_%ofAvg'] = data[f'{L2KPI2}_Trend'] / data[f'Avg_{L2KPI2}']\n    data[f'{L2KPI3}_Slope_%ofAvg'] = data[f'{L2KPI3}_Trend'] / data[f'Avg_{L2KPI3}']\n    data[f'{L1KPI}_Trend_Decomp'] = data[f'{L1KPI}_Weight'] * data[f'{L1KPI}_Trend']\n    data[f'{L2KPI1}_Trend_Decomp'] = data[f'{L2KPI1}_Weight'] * data[f'{L2KPI1}_Trend']\n    data[f'{L2KPI2}_Trend_Decomp'] = data[f'{L2KPI2}_Weight'] * data[f'{L2KPI2}_Trend']\n    data[f'{L2KPI3}_Trend_Decomp'] = data[f'{L2KPI3}_Weight'] * data[f'{L2KPI3}_Trend']\n\n    #Loops over each rolling window to calculate trend decomp and business mix\n    date = list(data['timestamp'].unique())\n    dates = []\n    for c in date:\n        locals()[c] = data.loc[data['timestamp'] == c]\n        dates.append(locals()[c])\n\n    for s in dates:\n        s[f'Agg_{L1KPI}_Trend_Decomp'] = s[f'{L1KPI}_Trend_Decomp'].sum()\n        s[f'Agg_{L2KPI1}_Trend_Decomp'] = s[f'{L2KPI1}_Trend_Decomp'].sum()\n        s[f'Agg_{L2KPI2}_Trend_Decomp'] = s[f'{L2KPI2}_Trend_Decomp'].sum()\n        s[f'Agg_{L2KPI3}_Trend_Decomp'] = s[f'{L2KPI3}_Trend_Decomp'].sum()\n        s[f'{L1KPI}_Business_Mix'] = kpiTrend - s[f'{L1KPI}_Trend_Decomp'].sum()\n        s[f'{L2KPI1}_Business_Mix'] = kpiTrend - s[f'{L2KPI1}_Trend_Decomp'].sum()\n        s[f'{L2KPI2}_Business_Mix'] = kpiTrend - s[f'{L2KPI2}_Trend_Decomp'].sum()\n        s[f'{L2KPI3}_Business_Mix'] = kpiTrend - s[f'{L2KPI3}_Trend_Decomp'].sum()\n\n    data = pd.concat(dates).fillna(0)\n\n    data[f'{L1KPI}_Attribution'] = data[f'{L1KPI}_Trend_Decomp'] / kpiTrend\n    data[f'{L2KPI1}_Attribution'] = data[f'{L2KPI1}_Trend_Decomp'] / kpiTrend\n    data[f'{L2KPI2}_Attribution'] = data[f'{L2KPI2}_Trend_Decomp'] / kpiTrend\n    data[f'{L2KPI3}_Attribution'] = data[f'{L2KPI3}_Trend_Decomp'] / kpiTrend\n    data[f'{L1KPI}_Impact_%{L1KPI}'] = data[f'{L1KPI}_Trend_Decomp'] / data[f'Avg_{L1KPI}']\n    data[f'{L2KPI1}_Impact_%{L2KPI1}'] = data[f'{L2KPI1}_Trend_Decomp'] / data[f'Avg_{L2KPI1}']\n    data[f'{L2KPI2}_Impact_%{L2KPI2}'] = data[f'{L2KPI2}_Trend_Decomp'] / data[f'Avg_{L2KPI2}']\n    data[f'{L2KPI3}_Impact_%{L2KPI3}'] = data[f'{L2KPI3}_Trend_Decomp'] / data[f'Avg_{L2KPI3}']\n\n    date = list(data['timestamp'].unique())\n    dates = []\n    for c in date:\n        locals()[c] = data.loc[data['timestamp'] == c]\n        dates.append(locals()[c])\n\n    for s in dates:\n        s[f'{L1KPI}_Att_Business_Mix'] = s[f'{L1KPI}_Business_Mix'] / kpiTrend\n        s[f'{L1KPI}_Att_Agg'] = s[f'{L1KPI}_Attribution'].sum() + s[f'{L1KPI}_Att_Business_Mix']\n        s[f'{L2KPI1}_Att_Business_Mix'] = s[f'{L2KPI1}_Business_Mix'] / kpiTrend\n        s[f'{L2KPI1}_Att_Agg'] = s[f'{L2KPI1}_Attribution'].sum() + s[f'{L2KPI1}_Att_Business_Mix']\n        s[f'{L2KPI2}_Att_Business_Mix'] = s[f'{L2KPI2}_Business_Mix'] / kpiTrend\n        s[f'{L2KPI2}_Att_Agg'] = s[f'{L2KPI2}_Attribution'].sum() + s[f'{L2KPI2}_Att_Business_Mix']\n        s[f'{L2KPI3}_Att_Business_Mix'] = s[f'{L2KPI3}_Business_Mix'] / kpiTrend\n        s[f'{L2KPI3}_Att_Agg'] = s[f'{L2KPI3}_Attribution'].sum() + s[f'{L2KPI3}_Att_Business_Mix']\n    data = pd.concat(dates).fillna(0)\n    #global kpiTrend1\n    #kpiTrend1 = data[f'{kpi1}_Trend']    \n    data[f'Agg_Trend_Decomp'] = data[f'{L1KPI}_Trend_Decomp'].sum()\n    result1 = data.loc[data['timestamp'] == an_date]\n    \n    #if level 2 is defined, the trend value for the user specified KPI will be calculated to be used for the next drilldown\n    if level2 != '':\n      global kpiTrend1\n      kpiTrend1 = result1[f'{kpi1}_Trend'].loc[result1[level1] == l1_items]\n      kpiTrend1 = kpiTrend1.iloc[0]\n      \n    result1 = result1[['timestamp', f'{level1}', f'{L1KPI}_Trend', f'{kpi1}_Trend_Decomp', f'Agg_{L1KPI}_Trend_Decomp', f'{kpi1}_Business_Mix', f'{kpi1}_Impact_%{kpi1}', f'{kpi1}_Attribution', f'{kpi1}_Att_Business_Mix', f'{kpi1}_Att_Agg']]\n    display(result1)\n    return(result1)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"db3be649-5c30-443a-9af6-a2a5605cfa0a"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["def L22_causality(df, file_path, level1,level2,level3,kpi1,start_date,end_date,period, baseline_window, field1, field2, field3, an_item1, an_item2, an_item3, L1KPI, L1Num, L1Den, num_function_1, den_function_1, L2KPI1, L2Num1, L2Den1, sub_num_function_1, sub_den_function_1, L2KPI2, L2Num2, L2Den2, sub_num_function_2, sub_den_function_2, L2KPI3, L2Num3, L2Den3, sub_num_function_3, sub_den_function_3, L2KPI4, L2Num4, L2Den4, sub_num_function_4, sub_den_function_4, an_date, kpiTrend, l1_items, l2_items, kpiTrend1, ranking_field, ranking_function, bins_cols, bins_size, date_field):\n  import glob\n  import os\n  from datetime import datetime, timedelta\n  #import pandasql as ps\n  from pyspark.sql import SQLContext\n  \n  sqlContext = SQLContext(sc)\n  import time\n  \n  #df = pd.read_csv('/dbfs/FileStore/CG/output_chubb/data.csv')\n  start = time.time()\n  print(\"file read from dbfs started\")\n  #Read part train files from DBFS and concatenate into dataframe  \n  #file path\n  #path = '/dbfs/sg_claims_files/'\n  path = file_path\n  # check if string \"claims\" is part of the filepath - to identify claims/loss ratio database\n  if 'claims' in path:\n    #Read part train files from DBFS and concatenate into dataframe\n    all_files = glob.glob(os.path.join(path, \"SG_Claims_Training_0.ftr\"))     \n    df_from_each_file = (pd.read_feather(f) for f in all_files) #all columns\n    concatenated_df   = pd.concat(df_from_each_file, ignore_index=True)\n    # calcualte claims count and policy_cvog count from raw data (since these fields are required to calculate some KPI's it has been hardcoded. Assumption is that these fields will be already available in the data later)\n    concatenated_df['CLAIMS_COUNT']= np.where((concatenated_df['ULT_CLAIM_INCURRED'])>0, concatenated_df['CLAIMNUMBER'] + concatenated_df['COVERAGE_GROUP'] , '999')\n    concatenated_df['POL_CVG_COUNT']= np.where((concatenated_df['POLICY']).notna(), concatenated_df['POLICY'] + concatenated_df['COVERAGE_GROUP'] , '999')\n  #for billing and other database files which are single files in .csv format  \n  else:\n    #read file\n    concatenated_df = pd.read_csv(path)\n  \n  #Convert date_field to pandas datetime\n  concatenated_df[date_field] = pd.to_datetime(concatenated_df[date_field])\n  # Date Filter\n  concatenated_df = concatenated_df.loc[(concatenated_df[date_field] >= start_date) & (concatenated_df[date_field] <= end_date)]\n  \n  #Binning columns with user selected column names and bin sizes\n  for c in range(len(bins_cols)):\n    concatenated_df[bins_cols[c]+'_bins'] = pd.qcut(concatenated_df[bins_cols[c]], q=bins_size[c], precision = 0).astype(str)\n  \n  #Add month and year columns\n  concatenated_df['Accd_month'] = pd.to_datetime(concatenated_df[date_field]).dt.month_name()\n  concatenated_df['Accd_year'] = pd.to_datetime(concatenated_df[date_field]).dt.year\n  concatenated_df['Accd_quarter'] = pd.to_datetime(concatenated_df[date_field]).dt.quarter\n  \n  #display(concatenated_df)\n  \n  end = time.time()\n  print(\"file read from dbfs ended\",end - start)\n  \n  #Filter data on values in the anomaly result case as per the fields selected in Anomaly Detection. \n  if((field1 !=\"\") & (field2!=\"\") & (field3!=\"\")):\n      concatenated_df =  concatenated_df[( concatenated_df[field1] == an_item1) & ( concatenated_df[field2] == an_item2) & ( concatenated_df[field3] == an_item3)]\n      \n      #display(df)\n  \n  elif((field1 !=\"\")   & (field2==\"\") & (field3==\"\")): \n      concatenated_df = concatenated_df.loc[concatenated_df[field1] == an_item1]\n      \n      #display(concatenated_df)\n  \n  elif((field1 !=\"\")   & (field2 !=\"\" ) & (field3==\"\")):\n     concatenated_df =  concatenated_df[( concatenated_df[field1] == an_item1) & ( concatenated_df[field2] == an_item2)]\n    \n    #display(concatenated_df)\n  \n  #l2 data preprocessing------------------------------------------------------------------------\n  #Filter on level 1 and level 2 field\n  \n  \n  data = concatenated_df\n  #put aggregation columns in a list\n  #put aggregation columns in a list\n  lst2 = [level1,level2]\n  #put columns for year and month in list\n  if(period == 'month'):\n      segment = ['Accd_year','Accd_month']\n      \n    #put columns for year and quarter in list\n  if(period == 'quarter'):\n      segment = ['Accd_year','Accd_quarter']\n      \n  segment.extend(lst2)\n  \n  \n  if((level1!='') & (level2!='')):\n    \n    #filter data on level1 values\n    concatenated_df = concatenated_df.loc[concatenated_df[level1] == l1_items]\n    \n\n    # calculate KPI values for KPIs selected by user\n    if((L1KPI !=\"\") & (L2KPI1 !=\"\") & (L2KPI2 !=\"\") & (L2KPI3 !=\"\")):\n      \n      data = concatenated_df.groupby(segment).agg({L1Num: num_function_1 , L1Den: den_function_1, L2Num1:sub_num_function_1, L2Den1: sub_den_function_1, L2Num2:sub_num_function_2, L2Den2:sub_den_function_2, L2Num3:sub_num_function_3, L2Den3:sub_den_function_3, L2Num4:sub_num_function_4, L2Den4:sub_den_function_4, ranking_field: ranking_function})\n      #data = concatenated_df.groupby(segment)\n      data.reset_index(inplace = True)\n      #display(data)\n      #if empty data set obtained after aggregation\n      if data.empty:\n        return('No records found for this selection!')\n    #Calculate values for KPI's for each row timeArray is used for the rolling average function. Creates an index with length defined by the baseline_window variable\n    data[L1KPI] = data[L1Num]/data[L1Den]\n    data[L2KPI1] = data[L2Num1]/data[L2Den1]\n    data[L2KPI2] = data[L2Num2]/data[L2Den2]\n    data[L2KPI3] = data[L2Num3]/data[L2Den3]\n    data[f'{L2KPI3}'] = data[f'{L2KPI3}'].astype(float)\n    data[f'{L2Den3}'] = data[f'{L2Num3}']/data[f'{L2KPI3}']\n    \n    #Create timestamp column\n    if(period == 'month'):\n      data['Month Name-Year']=pd.to_datetime(data['Accd_month']+data['Accd_year'].astype(str),format='%B%Y')\n      data['timestamp'] = data['Month Name-Year'].dt.strftime('%Y-%m')\n\n    if(period == 'quarter'):\n      data['Accd_quarter']=data['Accd_quarter'].replace({1:'1', 2: '4', 3: '7', 4: '10'})\n      data['quart_year']=pd.to_datetime(data['Accd_year'].astype(str) +data['Accd_quarter'].astype(str),format='%Y%m')\n      data['timestamp'] = data['quart_year'].dt.strftime('%Y-%m')\n      \n\n\n    data['timestamp'] = pd.to_datetime(data['timestamp'])\n    data =  data.sort_values(by='timestamp', ascending=True)\n    #Replace invalid values\n    data = data.replace('#DIV/0!', np.NaN)\n    data = data.replace('#VALUE!', np.NaN)\n    #Conversion to float\n    data[L1KPI] = data[L1KPI].astype(str).astype(float)\n    numeric_columns = data.select_dtypes(include=['number']).columns\n    from dateutil.relativedelta import relativedelta\n  \n    #Create a list of the dates within the rolling window used for causal analysis\n    if(period == 'month'):\n      an_date = pd.to_datetime(an_date)\n      numlist = list(range(0,baseline_window,1))\n      timelist = []\n      for i in numlist:\n        past_date = an_date - relativedelta(months=baseline_window - 1 - i)\n        timelist.append( datetime(past_date.year, past_date.month, 1).strftime('%Y-%m-%d') )\n      print(timelist)    \n      timeArray = np.array(timelist)\n      \n    if(period == 'quarter'):\n      an_date = pd.to_datetime(an_date)\n      numlist = list(range(0,(baseline_window*3),3))\n      timelist = []\n      for i in numlist:\n        past_date = an_date - relativedelta(months=(baseline_window*3) - 3 - i)\n        past_date = past_date.strftime('%Y-%m-%d')\n        timelist.append(past_date)\n      print(timelist)  \n      timeArray = np.array(timelist)\n    \n    dgroups = []\n    for d in timelist:\n        locals()[d] = data.loc[data['timestamp'] == d]\n        dgroups.append(locals()[d])\n    data = pd.concat(dgroups).fillna(0)\n    #display(data)\n    \n    #timeArray is used for the rolling average function. Creates an index with length defined by the baseline_window variable\n    timelist = list(range(0,baseline_window,1))\n    timeArray = np.array(timelist)\n    len(timeArray)\n    data = data.fillna(0)\n\n\n    group1 = list(data[level1].unique())\n    groups1 = []\n    for c in group1:\n        locals()[c] = data.loc[data[level1] == c]\n        groups1.append(locals()[c])\n        \n    group2 = list(data[level2].unique())\n    groups2 = []\n    for c in group2:\n        locals()[c] = data.loc[data[level2] == c]\n        groups2.append(locals()[c])\n        \n    for g in groups1:\n      for s in groups2:\n          Array = s[L1KPI].to_numpy()\n          Trend = []\n          for i in range(0, len(Array), 1):\n              if i>(baseline_window-2):\n                  x = timeArray\n                  y = Array[timeArray-(baseline_window-1)+i]\n                  m, b = np.polyfit(x, y, 1)\n                  Trend.append(m)\n              else:\n                  Trend.append(0)\n\n          Trend = np.asarray(Trend)\n          s[f'{L1KPI}_Trend'] = Trend\n\n          #Weighting basis\n          s[f'Avg_{L1KPI}'] = s[f'{L1KPI}'].rolling(window = baseline_window).mean()\n          s[f'Avg_{L2KPI1}'] = s[f'{L2KPI1}'].rolling(window = baseline_window).mean()\n          s[f'Avg_{L2KPI2}'] = s[f'{L2KPI2}'].rolling(window = baseline_window).mean()\n          s[f'Avg_{L2KPI3}'] = s[f'{L2KPI3}'].rolling(window = baseline_window).mean()\n          s[f'Avg_{L2Num2}'] = s[f'{L2Num2}'].rolling(window = baseline_window).mean()\n          s[f'Avg_{L2Den2}'] = s[f'{L2Den2}'].rolling(window = baseline_window).mean()\n          s[f'Avg_{L2Den3}'] = s[f'{L2Den3}'].rolling(window = baseline_window).mean()\n\n          #Calculate slope for {L2KPI1} metric\n          Array = s[f'{L2KPI1}'].to_numpy()\n          Trend = []\n          for i in range(0, len(Array), 1):\n              if i>(baseline_window-2):\n                  x = timeArray\n                  y = Array[timeArray-(baseline_window-1)+i]\n                  m, b = np.polyfit(x, y, 1)\n                  Trend.append(m)\n              else:\n                  Trend.append(0)\n          Trend = np.asarray(Trend)\n          s[f'{L2KPI1}_Trend'] = Trend\n\n          #Calculate slope for {L2KPI2} metric\n          Array = s[f'{L2KPI2}'].to_numpy()\n          Trend = []\n          for i in range(0, len(Array), 1):\n              if i>(baseline_window-2):\n                  x = timeArray\n                  y = Array[timeArray-(baseline_window-1)+i]\n                  m, b = np.polyfit(x, y, 1)\n                  Trend.append(m)\n              else:\n                  Trend.append(0)\n\n          Trend = np.asarray(Trend)\n          s[f'{L2KPI2}_Trend'] = Trend\n\n          #Calculate slope for {L2KPI3} metric\n          Array = s[f'{L2KPI3}'].to_numpy()\n          Trend = []\n          for i in range(0, len(Array), 1):\n              if i>(baseline_window-2):\n                  x = timeArray\n                  y = Array[timeArray-(baseline_window-1)+i]\n                  m, b = np.polyfit(x, y, 1)\n                  Trend.append(m)\n              else:\n                  Trend.append(0)\n\n          Trend = np.asarray(Trend)\n          s[f'{L2KPI3}_Trend'] = Trend\n        #data = pd.concat(groups2).fillna(0)\n      data = pd.concat(groups2).fillna(0)\n\n      Array = s[L1KPI].to_numpy()\n      Trend = []\n      for i in range(0, len(Array), 1):\n          if i>(baseline_window-2):\n              x = timeArray\n              y = Array[timeArray-(baseline_window-1)+i]\n              m, b = np.polyfit(x, y, 1)\n              Trend.append(m)\n          else:\n              Trend.append(0)\n\n      Trend = np.asarray(Trend)\n      s[f'{L1KPI}_Trend'] = Trend\n      \n    #display(data)\n\n    date = list(data['timestamp'].unique())\n    dates = []\n    for c in date:\n        locals()[c] = data.loc[data['timestamp'] == c]\n        dates.append(locals()[c])\n\n    #Loops over each rolling window and calculates the weights for each occurance\n    for s in dates:\n        Array = s[f'Avg_{L2KPI3}'].to_numpy()\n        s[f'{L1KPI}_weight_basis'] = Array.sum()\n\n        Array = s[f'Avg_{L2Num2}'].to_numpy()\n        s[f'{L2KPI1}_weight_basis'] = Array.sum()\n\n        Array = s[f'Avg_{L2Den2}'].to_numpy()\n        s[f'{L2KPI2}_weight_basis'] = Array.sum()\n\n        Array = s[f'Avg_{L2Den3}'].to_numpy()\n        s[f'{L2KPI3}_weight_basis'] = Array.sum()\n    data = pd.concat(dates).fillna(0)\n\n    data[f'{L1KPI}_Weight'] = data[f'Avg_{L2KPI3}'] / data[f'{L1KPI}_weight_basis']\n    data[f'{L2KPI1}_Weight'] = data[f'Avg_{L2Num2}'] / data[f'{L2KPI1}_weight_basis']\n    data[f'{L2KPI2}_Weight'] = data[f'Avg_{L2Den2}'] / data[f'{L2KPI2}_weight_basis']\n    data[f'{L2KPI3}_Weight'] = data[f'Avg_{L2Den3}'] / data[f'{L2KPI3}_weight_basis']\n\n    #Calculates aggregated weights to ensure it adds up to exactly 1\n    data1 = data.loc[data['timestamp'] == an_date]\n    data1[f'Agg_Weight'] = data1[f'{L1KPI}_Weight'].sum()\n\n    data[f'{L1KPI}_Slope_%ofAvg'] = data[f'{L1KPI}_Trend'] / data[f'Avg_{L1KPI}']\n    data[f'{L2KPI1}_Slope_%ofAvg'] = data[f'{L2KPI1}_Trend'] / data[f'Avg_{L2KPI1}']\n    data[f'{L2KPI2}_Slope_%ofAvg'] = data[f'{L2KPI2}_Trend'] / data[f'Avg_{L2KPI2}']\n    data[f'{L2KPI3}_Slope_%ofAvg'] = data[f'{L2KPI3}_Trend'] / data[f'Avg_{L2KPI3}']\n    data[f'{L1KPI}_Trend_Decomp'] = data[f'{L1KPI}_Weight'] * data[f'{L1KPI}_Trend']\n    data[f'{L2KPI1}_Trend_Decomp'] = data[f'{L2KPI1}_Weight'] * data[f'{L2KPI1}_Trend']\n    data[f'{L2KPI2}_Trend_Decomp'] = data[f'{L2KPI2}_Weight'] * data[f'{L2KPI2}_Trend']\n    data[f'{L2KPI3}_Trend_Decomp'] = data[f'{L2KPI3}_Weight'] * data[f'{L2KPI3}_Trend']\n\n    #Loops over each rolling window to calculate trend decomp and business mix\n    date = list(data['timestamp'].unique())\n    dates = []\n    for c in date:\n        locals()[c] = data.loc[data['timestamp'] == c]\n        dates.append(locals()[c])\n\n    for s in dates:\n        s[f'Agg_{L1KPI}_Trend_Decomp'] = s[f'{L1KPI}_Trend_Decomp'].sum()\n        s[f'Agg_{L2KPI1}_Trend_Decomp'] = s[f'{L2KPI1}_Trend_Decomp'].sum()\n        s[f'Agg_{L2KPI2}_Trend_Decomp'] = s[f'{L2KPI2}_Trend_Decomp'].sum()\n        s[f'Agg_{L2KPI3}_Trend_Decomp'] = s[f'{L2KPI3}_Trend_Decomp'].sum()\n        s[f'{L1KPI}_Business_Mix'] = kpiTrend1 - s[f'{L1KPI}_Trend_Decomp'].sum()\n        s[f'{L2KPI1}_Business_Mix'] = kpiTrend1 - s[f'{L2KPI1}_Trend_Decomp'].sum()\n        s[f'{L2KPI2}_Business_Mix'] = kpiTrend1 - s[f'{L2KPI2}_Trend_Decomp'].sum()\n        s[f'{L2KPI3}_Business_Mix'] = kpiTrend1 - s[f'{L2KPI3}_Trend_Decomp'].sum()\n\n    data = pd.concat(dates).fillna(0)\n\n    data[f'{L1KPI}_Attribution'] = data[f'{L1KPI}_Trend_Decomp'] / kpiTrend1\n    data[f'{L2KPI1}_Attribution'] = data[f'{L2KPI1}_Trend_Decomp'] / kpiTrend1\n    data[f'{L2KPI2}_Attribution'] = data[f'{L2KPI2}_Trend_Decomp'] / kpiTrend1\n    data[f'{L2KPI3}_Attribution'] = data[f'{L2KPI3}_Trend_Decomp'] / kpiTrend1\n    data[f'{L1KPI}_Impact_%{L1KPI}'] = data[f'{L1KPI}_Trend_Decomp'] / data[f'Avg_{L1KPI}']\n    data[f'{L2KPI1}_Impact_%{L2KPI1}'] = data[f'{L2KPI1}_Trend_Decomp'] / data[f'Avg_{L2KPI1}']\n    data[f'{L2KPI2}_Impact_%{L2KPI2}'] = data[f'{L2KPI2}_Trend_Decomp'] / data[f'Avg_{L2KPI2}']\n    data[f'{L2KPI3}_Impact_%{L2KPI3}'] = data[f'{L2KPI3}_Trend_Decomp'] / data[f'Avg_{L2KPI3}']\n\n    date = list(data['timestamp'].unique())\n    dates = []\n    for c in date:\n        locals()[c] = data.loc[data['timestamp'] == c]\n        dates.append(locals()[c])\n\n    for s in dates:\n        s[f'{L1KPI}_Att_Business_Mix'] = s[f'{L1KPI}_Business_Mix'] / kpiTrend1\n        s[f'{L1KPI}_Att_Agg'] = s[f'{L1KPI}_Attribution'].sum() + s[f'{L1KPI}_Att_Business_Mix']\n        s[f'{L2KPI1}_Att_Business_Mix'] = s[f'{L2KPI1}_Business_Mix'] / kpiTrend1\n        s[f'{L2KPI1}_Att_Agg'] = s[f'{L2KPI1}_Attribution'].sum() + s[f'{L2KPI1}_Att_Business_Mix']\n        s[f'{L2KPI2}_Att_Business_Mix'] = s[f'{L2KPI2}_Business_Mix'] / kpiTrend1\n        s[f'{L2KPI2}_Att_Agg'] = s[f'{L2KPI2}_Attribution'].sum() + s[f'{L2KPI2}_Att_Business_Mix']\n        s[f'{L2KPI3}_Att_Business_Mix'] = s[f'{L2KPI3}_Business_Mix'] / kpiTrend1\n        s[f'{L2KPI3}_Att_Agg'] = s[f'{L2KPI3}_Attribution'].sum() + s[f'{L2KPI3}_Att_Business_Mix']\n    data = pd.concat(dates).fillna(0)\n    #global kpiTrend2\n    #kpiTrend2 = data[f'{kpi1}_Trend']\n    #data1 = data.loc[data['timestamp'] == '2019-04']\n    data[f'Agg_Trend_Decomp'] = data[f'{L1KPI}_Trend_Decomp'].sum()\n    #data2 = data.loc[data['timestamp'] == an_date]\n    #display(data2)\n    result2 = data.loc[(data['timestamp'] == an_date) & (data[level1] == l1_items)]\n    \n    #if level 3 is defined, the trend value for the user specified KPI will be calculated to be used for the next drilldown\n    if level3 != '':\n      global kpiTrend2\n      kpiTrend2 = result2[f'{kpi1}_Trend'].loc[result2[level2] == l2_items]\n      kpiTrend2 = kpiTrend2.iloc[0]\n    result2 = result2[['timestamp', f'{level2}',f'{level1}', f'{kpi1}_Trend', f'{kpi1}_Trend_Decomp',f'{kpi1}_Business_Mix', f'{kpi1}_Impact_%{kpi1}', f'{kpi1}_Attribution', f'{kpi1}_Att_Business_Mix', f'{kpi1}_Att_Agg']]\n    display(result2)\n    return(result2)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6c1a5300-1897-48e3-8932-940a09d4ff5d"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["def L23_causality(df, file_path, level1,level2,level3,kpi1,start_date,end_date,period, baseline_window, field1, field2, field3, an_item1, an_item2, an_item3, L1KPI, L1Num, L1Den, num_function_1, den_function_1, L2KPI1, L2Num1, L2Den1, sub_num_function_1, sub_den_function_1, L2KPI2, L2Num2, L2Den2, sub_num_function_2, sub_den_function_2, L2KPI3, L2Num3, L2Den3, sub_num_function_3, sub_den_function_3, L2KPI4, L2Num4, L2Den4, sub_num_function_4, sub_den_function_4, an_date, kpiTrend, l1_items, l2_items, kpiTrend2, ranking_field, ranking_function, bins_cols, bins_size, date_field):\n  import glob\n  import os\n  from datetime import datetime, timedelta\n  #import pandasql as ps\n  from pyspark.sql import SQLContext\n  \n  sqlContext = SQLContext(sc)\n  import time\n  \n  #df = pd.read_csv('/dbfs/FileStore/CG/output_chubb/data.csv')\n  start = time.time()\n  print(\"file read from dbfs started\")\n  #Read part train files from DBFS and concatenate into dataframe  \n  #file path\n  #path = '/dbfs/sg_claims_files/'\n  path = file_path\n  # check if string \"claims\" is part of the filepath - to identify claims/loss ratio database\n  if 'claims' in path:\n    #Read part train files from DBFS and concatenate into dataframe\n    all_files = glob.glob(os.path.join(path, \"SG_Claims_Training_0.ftr\"))     \n    df_from_each_file = (pd.read_feather(f) for f in all_files) #all columns\n    concatenated_df   = pd.concat(df_from_each_file, ignore_index=True)\n    # calcualte claims count and policy_cvog count from raw data (since these fields are required to calculate some KPI's it has been hardcoded. Assumption is that these fields will be already available in the data later)\n    concatenated_df['CLAIMS_COUNT']= np.where((concatenated_df['ULT_CLAIM_INCURRED'])>0, concatenated_df['CLAIMNUMBER'] + concatenated_df['COVERAGE_GROUP'] , '999')\n    concatenated_df['POL_CVG_COUNT']= np.where((concatenated_df['POLICY']).notna(), concatenated_df['POLICY'] + concatenated_df['COVERAGE_GROUP'] , '999')\n  #for billing and other database files which are single files in .csv format  \n  else:\n    #read file\n    concatenated_df = pd.read_csv(path)\n  \n  #Convert date_field to pandas datetime\n  concatenated_df[date_field] = pd.to_datetime(concatenated_df[date_field])\n  # Date Filter\n  concatenated_df = concatenated_df.loc[(concatenated_df[date_field] >= start_date) & (concatenated_df[date_field] <= end_date)]\n  \n  #Binning columns with user selected column names and bin sizes\n  for c in range(len(bins_cols)):\n    concatenated_df[bins_cols[c]+'_bins'] = pd.qcut(concatenated_df[bins_cols[c]], q=bins_size[c], precision = 0).astype(str)\n  \n  #Add month and year columns\n  concatenated_df['Accd_month'] = pd.to_datetime(concatenated_df[date_field]).dt.month_name()\n  concatenated_df['Accd_year'] = pd.to_datetime(concatenated_df[date_field]).dt.year\n  concatenated_df['Accd_quarter'] = pd.to_datetime(concatenated_df[date_field]).dt.quarter\n  \n  #display(concatenated_df)\n  \n  end = time.time()\n  print(\"file read from dbfs ended\",end - start)\n  \n  #Filter data on values in the anomaly result case as per the fields selected in Anomaly Detection. \n  if((field1 !=\"\") & (field2!=\"\") & (field3!=\"\")):\n      concatenated_df =  concatenated_df[( concatenated_df[field1] == an_item1) & ( concatenated_df[field2] == an_item2) & ( concatenated_df[field3] == an_item3)]\n      \n      #display(df)\n  \n  elif((field1 !=\"\")   & (field2==\"\") & (field3==\"\")): \n      concatenated_df = concatenated_df.loc[concatenated_df[field1] == an_item1]\n      \n      #display(concatenated_df)\n  \n  elif((field1 !=\"\")   & (field2 !=\"\" ) & (field3==\"\")):\n     concatenated_df =  concatenated_df[( concatenated_df[field1] == an_item1) & ( concatenated_df[field2] == an_item2)]\n    \n    #display(concatenated_df)\n  \n  #l2 data preprocessing------------------------------------------------------------------------\n  #Filter on level 1 and level 2 field\n  \n  \n  data = concatenated_df\n  #put aggregation columns in a list\n  #put aggregation columns in a list\n  lst3 = [level1,level2,level3]\n  #put columns for year and month in list\n  if(period == 'month'):\n      segment = ['Accd_year','Accd_month']\n      \n    #put columns for year and quarter in list\n  if(period == 'quarter'):\n      segment = ['Accd_year','Accd_quarter']\n      \n  segment.extend(lst3)\n  \n  \n  if((level1!='') & (level2!='') & (level3!='')):\n    \n    #filter data on level1 values\n    concatenated_df = concatenated_df.loc[(concatenated_df[level1] == l1_items) & (concatenated_df[level2] == l2_items)]\n    \n\n    # calculate KPI values for KPIs selected by user\n    if((L1KPI !=\"\") & (L2KPI1 !=\"\") & (L2KPI2 !=\"\") & (L2KPI3 !=\"\")):\n      \n      data = concatenated_df.groupby(segment).agg({L1Num: num_function_1 , L1Den: den_function_1, L2Num1:sub_num_function_1, L2Den1: sub_den_function_1, L2Num2:sub_num_function_2, L2Den2:sub_den_function_2, L2Num3:sub_num_function_3, L2Den3:sub_den_function_3, L2Num4:sub_num_function_4, L2Den4:sub_den_function_4, ranking_field: ranking_function})\n      #data = concatenated_df.groupby(segment)\n      data.reset_index(inplace = True)\n      #display(data)\n      #if empty data set obtained after aggregation\n      if data.empty:\n        return('No records found for this selection!')\n    #Calculate values for KPI's for each row timeArray is used for the rolling average function. Creates an index with length defined by the baseline_window variable\n    data[L1KPI] = data[L1Num]/data[L1Den]\n    data[L2KPI1] = data[L2Num1]/data[L2Den1]\n    data[L2KPI2] = data[L2Num2]/data[L2Den2]\n    data[L2KPI3] = data[L2Num3]/data[L2Den3]\n    data[f'{L2KPI3}'] = data[f'{L2KPI3}'].astype(float)\n    data[f'{L2Den3}'] = data[f'{L2Num3}']/data[f'{L2KPI3}']\n    \n    #Create timestamp column\n    if(period == 'month'):\n      data['Month Name-Year']=pd.to_datetime(data['Accd_month']+data['Accd_year'].astype(str),format='%B%Y')\n      data['timestamp'] = data['Month Name-Year'].dt.strftime('%Y-%m')\n\n    if(period == 'quarter'):\n      data['Accd_quarter']=data['Accd_quarter'].replace({1:'1', 2: '4', 3: '7', 4: '10'})\n      data['quart_year']=pd.to_datetime(data['Accd_year'].astype(str) +data['Accd_quarter'].astype(str),format='%Y%m')\n      data['timestamp'] = data['quart_year'].dt.strftime('%Y-%m')\n      \n\n\n    data['timestamp'] = pd.to_datetime(data['timestamp'])\n    data =  data.sort_values(by='timestamp', ascending=True)\n    #Replace invalid values\n    data = data.replace('#DIV/0!', np.NaN)\n    data = data.replace('#VALUE!', np.NaN)\n    #Conversion to float\n    data[L1KPI] = data[L1KPI].astype(str).astype(float)\n    numeric_columns = data.select_dtypes(include=['number']).columns\n    from dateutil.relativedelta import relativedelta\n    \n    #Create a list of the dates within the rolling window used for causal analysis\n    if(period == 'month'):\n      an_date = pd.to_datetime(an_date)\n      numlist = list(range(0,baseline_window,1))\n      timelist = []\n      for i in numlist:\n        past_date = an_date - relativedelta(months=baseline_window - 1 - i)\n        timelist.append( datetime(past_date.year, past_date.month, 1).strftime('%Y-%m-%d') )\n      print(timelist)    \n      timeArray = np.array(timelist)\n      \n    if(period == 'quarter'):\n      an_date = pd.to_datetime(an_date)\n      numlist = list(range(0,(baseline_window*3),3))\n      timelist = []\n      for i in numlist:\n        past_date = an_date - relativedelta(months=(baseline_window*3) - 3 - i)\n        past_date = past_date.strftime('%Y-%m-%d')\n        timelist.append(past_date)\n      print(timelist)  \n      timeArray = np.array(timelist)\n    \n    dgroups = []\n    for d in timelist:\n        locals()[d] = data.loc[data['timestamp'] == d]\n        dgroups.append(locals()[d])\n    data = pd.concat(dgroups).fillna(0)\n    #display(data)\n    \n    #timeArray is used for the rolling average function. Creates an index with length defined by the \n    #baseline_window variable\n    timelist = list(range(0,baseline_window,1))\n    timeArray = np.array(timelist)\n    len(timeArray)\n    data = data.fillna(0)\n\n    group1 = list(data[level1].unique())\n    groups1 = []\n    for c in group1:\n        locals()[c] = data.loc[data[level1] == c]\n        groups1.append(locals()[c])\n        \n    group2 = list(data[level2].unique())\n    groups2 = []\n    for c in group2:\n        locals()[c] = data.loc[data[level2] == c]\n        groups2.append(locals()[c])\n        \n    group3 = list(data[level3].unique())\n    groups3 = []\n    for c in group3:\n        locals()[c] = data.loc[data[level3] == c]\n        groups3.append(locals()[c])\n    \n    for b in group1:    \n      for g in groups2:\n        for s in groups3:\n            Array = s[L1KPI].to_numpy()\n            Trend = []\n            for i in range(0, len(Array), 1):\n                if i>(baseline_window-2):\n                    x = timeArray\n                    y = Array[timeArray-(baseline_window-1)+i]\n                    m, b = np.polyfit(x, y, 1)\n                    Trend.append(m)\n                else:\n                    Trend.append(0)\n\n            Trend = np.asarray(Trend)\n            s[f'{L1KPI}_Trend'] = Trend\n\n            #Weighting basis\n            s[f'Avg_{L1KPI}'] = s[f'{L1KPI}'].rolling(window = baseline_window).mean()\n            s[f'Avg_{L2KPI1}'] = s[f'{L2KPI1}'].rolling(window = baseline_window).mean()\n            s[f'Avg_{L2KPI2}'] = s[f'{L2KPI2}'].rolling(window = baseline_window).mean()\n            s[f'Avg_{L2KPI3}'] = s[f'{L2KPI3}'].rolling(window = baseline_window).mean()\n            s[f'Avg_{L2Num2}'] = s[f'{L2Num2}'].rolling(window = baseline_window).mean()\n            s[f'Avg_{L2Den2}'] = s[f'{L2Den2}'].rolling(window = baseline_window).mean()\n            s[f'Avg_{L2Den3}'] = s[f'{L2Den3}'].rolling(window = baseline_window).mean()\n\n            #Calculate slope for {L2KPI1} metric\n            Array = s[f'{L2KPI1}'].to_numpy()\n            Trend = []\n            for i in range(0, len(Array), 1):\n                if i>(baseline_window-2):\n                    x = timeArray\n                    y = Array[timeArray-(baseline_window-1)+i]\n                    m, b = np.polyfit(x, y, 1)\n                    Trend.append(m)\n                else:\n                    Trend.append(0)\n            Trend = np.asarray(Trend)\n            s[f'{L2KPI1}_Trend'] = Trend\n\n            #Calculate slope for {L2KPI2} metric\n            Array = s[f'{L2KPI2}'].to_numpy()\n            Trend = []\n            for i in range(0, len(Array), 1):\n                if i>(baseline_window-2):\n                    x = timeArray\n                    y = Array[timeArray-(baseline_window-1)+i]\n                    m, b = np.polyfit(x, y, 1)\n                    Trend.append(m)\n                else:\n                    Trend.append(0)\n\n            Trend = np.asarray(Trend)\n            s[f'{L2KPI2}_Trend'] = Trend\n\n            #Calculate slope for {L2KPI3} metric\n            Array = s[f'{L2KPI3}'].to_numpy()\n            Trend = []\n            for i in range(0, len(Array), 1):\n                if i>(baseline_window-2):\n                    x = timeArray\n                    y = Array[timeArray-(baseline_window-1)+i]\n                    m, b = np.polyfit(x, y, 1)\n                    Trend.append(m)\n                else:\n                    Trend.append(0)\n\n            Trend = np.asarray(Trend)\n            s[f'{L2KPI3}_Trend'] = Trend\n        data = pd.concat(groups3).fillna(0)\n\n        Array = s[L1KPI].to_numpy()\n        Trend = []\n        for i in range(0, len(Array), 1):\n            if i>(baseline_window-2):\n                x = timeArray\n                y = Array[timeArray-(baseline_window-1)+i]\n                m, b = np.polyfit(x, y, 1)\n                Trend.append(m)\n            else:\n                Trend.append(0)\n\n        Trend = np.asarray(Trend)\n        s[f'{L1KPI}_Trend'] = Trend\n\n    date = list(data['timestamp'].unique())\n    dates = []\n    for c in date:\n        locals()[c] = data.loc[data['timestamp'] == c]\n        dates.append(locals()[c])\n\n    #Loops over each rolling window and calculates the weights for each occurance\n    for s in dates:\n        Array = s[f'Avg_{L2KPI3}'].to_numpy()\n        s[f'{L1KPI}_weight_basis'] = Array.sum()\n\n        Array = s[f'Avg_{L2Num2}'].to_numpy()\n        s[f'{L2KPI1}_weight_basis'] = Array.sum()\n\n        Array = s[f'Avg_{L2Den2}'].to_numpy()\n        s[f'{L2KPI2}_weight_basis'] = Array.sum()\n\n        Array = s[f'Avg_{L2Den3}'].to_numpy()\n        s[f'{L2KPI3}_weight_basis'] = Array.sum()\n    data = pd.concat(dates).fillna(0)\n\n    data[f'{L1KPI}_Weight'] = data[f'Avg_{L2KPI3}'] / data[f'{L1KPI}_weight_basis']\n    data[f'{L2KPI1}_Weight'] = data[f'Avg_{L2Num2}'] / data[f'{L2KPI1}_weight_basis']\n    data[f'{L2KPI2}_Weight'] = data[f'Avg_{L2Den2}'] / data[f'{L2KPI2}_weight_basis']\n    data[f'{L2KPI3}_Weight'] = data[f'Avg_{L2Den3}'] / data[f'{L2KPI3}_weight_basis']\n\n    #Calculates aggregated weights to ensure it adds up to exactly 1\n    data1 = data.loc[data['timestamp'] == an_date]\n    data1[f'Agg_Weight'] = data1[f'{L1KPI}_Weight'].sum()\n\n    data[f'{L1KPI}_Slope_%ofAvg'] = data[f'{L1KPI}_Trend'] / data[f'Avg_{L1KPI}']\n    data[f'{L2KPI1}_Slope_%ofAvg'] = data[f'{L2KPI1}_Trend'] / data[f'Avg_{L2KPI1}']\n    data[f'{L2KPI2}_Slope_%ofAvg'] = data[f'{L2KPI2}_Trend'] / data[f'Avg_{L2KPI2}']\n    data[f'{L2KPI3}_Slope_%ofAvg'] = data[f'{L2KPI3}_Trend'] / data[f'Avg_{L2KPI3}']\n    data[f'{L1KPI}_Trend_Decomp'] = data[f'{L1KPI}_Weight'] * data[f'{L1KPI}_Trend']\n    data[f'{L2KPI1}_Trend_Decomp'] = data[f'{L2KPI1}_Weight'] * data[f'{L2KPI1}_Trend']\n    data[f'{L2KPI2}_Trend_Decomp'] = data[f'{L2KPI2}_Weight'] * data[f'{L2KPI2}_Trend']\n    data[f'{L2KPI3}_Trend_Decomp'] = data[f'{L2KPI3}_Weight'] * data[f'{L2KPI3}_Trend']\n\n    #Loops over each rolling window to calculate trend decomp and business mix\n    date = list(data['timestamp'].unique())\n    dates = []\n    for c in date:\n        locals()[c] = data.loc[data['timestamp'] == c]\n        dates.append(locals()[c])\n\n    for s in dates:\n        s[f'Agg_{L1KPI}_Trend_Decomp'] = s[f'{L1KPI}_Trend_Decomp'].sum()\n        s[f'Agg_{L2KPI1}_Trend_Decomp'] = s[f'{L2KPI1}_Trend_Decomp'].sum()\n        s[f'Agg_{L2KPI2}_Trend_Decomp'] = s[f'{L2KPI2}_Trend_Decomp'].sum()\n        s[f'Agg_{L2KPI3}_Trend_Decomp'] = s[f'{L2KPI3}_Trend_Decomp'].sum()\n        s[f'{L1KPI}_Business_Mix'] = kpiTrend2 - s[f'{L1KPI}_Trend_Decomp'].sum()\n        s[f'{L2KPI1}_Business_Mix'] = kpiTrend2 - s[f'{L2KPI1}_Trend_Decomp'].sum()\n        s[f'{L2KPI2}_Business_Mix'] = kpiTrend2 - s[f'{L2KPI2}_Trend_Decomp'].sum()\n        s[f'{L2KPI3}_Business_Mix'] = kpiTrend2 - s[f'{L2KPI3}_Trend_Decomp'].sum()\n\n    data = pd.concat(dates).fillna(0)\n\n    data[f'{L1KPI}_Attribution'] = data[f'{L1KPI}_Trend_Decomp'] / kpiTrend2\n    data[f'{L2KPI1}_Attribution'] = data[f'{L2KPI1}_Trend_Decomp'] / kpiTrend2\n    data[f'{L2KPI2}_Attribution'] = data[f'{L2KPI2}_Trend_Decomp'] / kpiTrend2\n    data[f'{L2KPI3}_Attribution'] = data[f'{L2KPI3}_Trend_Decomp'] / kpiTrend2\n    data[f'{L1KPI}_Impact_%{L1KPI}'] = data[f'{L1KPI}_Trend_Decomp'] / data[f'Avg_{L1KPI}']\n    data[f'{L2KPI1}_Impact_%{L2KPI1}'] = data[f'{L2KPI1}_Trend_Decomp'] / data[f'Avg_{L2KPI1}']\n    data[f'{L2KPI2}_Impact_%{L2KPI2}'] = data[f'{L2KPI2}_Trend_Decomp'] / data[f'Avg_{L2KPI2}']\n    data[f'{L2KPI3}_Impact_%{L2KPI3}'] = data[f'{L2KPI3}_Trend_Decomp'] / data[f'Avg_{L2KPI3}']\n\n    date = list(data['timestamp'].unique())\n    dates = []\n    for c in date:\n        locals()[c] = data.loc[data['timestamp'] == c]\n        dates.append(locals()[c])\n\n    for s in dates:\n        s[f'{L1KPI}_Att_Business_Mix'] = s[f'{L1KPI}_Business_Mix'] / kpiTrend2\n        s[f'{L1KPI}_Att_Agg'] = s[f'{L1KPI}_Attribution'].sum() + s[f'{L1KPI}_Att_Business_Mix']\n        s[f'{L2KPI1}_Att_Business_Mix'] = s[f'{L2KPI1}_Business_Mix'] / kpiTrend2\n        s[f'{L2KPI1}_Att_Agg'] = s[f'{L2KPI1}_Attribution'].sum() + s[f'{L2KPI1}_Att_Business_Mix']\n        s[f'{L2KPI2}_Att_Business_Mix'] = s[f'{L2KPI2}_Business_Mix'] / kpiTrend2\n        s[f'{L2KPI2}_Att_Agg'] = s[f'{L2KPI2}_Attribution'].sum() + s[f'{L2KPI2}_Att_Business_Mix']\n        s[f'{L2KPI3}_Att_Business_Mix'] = s[f'{L2KPI3}_Business_Mix'] / kpiTrend2\n        s[f'{L2KPI3}_Att_Agg'] = s[f'{L2KPI3}_Attribution'].sum() + s[f'{L2KPI3}_Att_Business_Mix']\n    data = pd.concat(dates).fillna(0)\n    #data = data.loc[data['timestamp'] == '2019-04']\n    data[f'Agg_Trend_Decomp'] = data[f'{L1KPI}_Trend_Decomp'].sum()    \n    result3 = data.loc[(data['timestamp'] == an_date) & (data[level1] == l1_items) & (data[level2] == l2_items)]\n    result3 = result3[['timestamp', f'{level3}', f'{kpi1}_Trend', f'{kpi1}_Trend_Decomp',f'{kpi1}_Business_Mix', f'{kpi1}_Impact_%{kpi1}', f'{kpi1}_Attribution', f'{kpi1}_Att_Business_Mix', f'{kpi1}_Att_Agg']]\n    display(result3)\n    return(result3)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"44f01b02-2f8a-4017-b5c1-5c3926621d8b"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["\nfrom datetime import datetime\nfrom dateutil.relativedelta import relativedelta\n\n\n\n#calculate start date of baseline based on window size\ndef get_baseline_date(period,date, window):\n  if(period == 'month'):\n    #Difference in months between anomaly date and baseline date defined by window\n    temp = datetime.strptime(date, '%Y-%m-%d')- relativedelta(months=window)\n    temp = format(temp,'%Y-%m-%d')\n    return temp\n  \n  elif(period == 'quarter'):\n    #print(\"inside date quarter\")\n    #Difference in quarters between anomaly date and baseline date defined by window\n    temp = datetime.strptime(date, '%Y-%m-%d')- relativedelta(months=window*3)\n    temp = format(temp,'%Y-%m-%d')\n    return temp\n\n\ndef main(file_path, baseline_window, L1KPI, L1Num, L1Den, num_function_1 , den_function_1, L2KPI1, L2Num1, L2Den1, sub_num_function_1 , sub_den_function_1, L2KPI2, L2Num2, L2Den2, sub_num_function_2 , sub_den_function_2, L2KPI3, L2Num3, L2Den3, sub_num_function_3 , sub_den_function_3, L2KPI4, L2Num4, L2Den4, sub_num_function_4 , sub_den_function_4, kpi1, ranking_field, ranking_function, level1, level2,level3, field1, field2, field3,start_date,end_date, period, an_date, an_item1, an_item2, an_item3, l1_items, l2_items, bins_cols, bins_size, date_field):\n  import glob\n  import os\n  #import pandasql as ps\n  from pyspark.sql import SQLContext\n  pd.set_option('display.max_columns', None)\n  #Read Anomaly Detection Result File\n\n  df = pd.read_csv('/dbfs/FileStore/CG/output_chubb/data2.csv')\n  result = pd.read_csv('/dbfs/FileStore/CG/output_chubb/anomalies2.csv')\n  kpiTrend = df.loc[df[field1] == an_item1]\n  kpiTrend = df.loc[df['timestamp'] == an_date]\n  kpiTrend = kpiTrend.iloc[0]['Trend']\n  print(kpiTrend)\n  #call L1 causality function\n  \n  \n  #Call the drill down functions according to the configuration of user defined KPI's and drill down levels\n  if((L2KPI1 !=\"\")  & (L2KPI2!=\"\") & (L2KPI3!=\"\") & (L2KPI4 != \"\")):\n    \n    temp1= L1_causality(df, result, level1,level2,level3,kpi1,start_date,end_date,period, baseline_window, field1, field2, field3, L1KPI, L1Num, L1Den, L2KPI1, L2Num1, L2Den1, L2KPI2, L2Num2, L2Den2, L2KPI3, L2Num3, L2Den3, L2KPI4, L2Num4, L2Den4, an_date, an_item1, an_item2, an_item3)\n    \n    if((level1 !=\"\")   & (level2==\"\") & (level3==\"\")):\n\n      l21 = L21_causality(df, file_path, level1,level2,level3,kpi1,start_date,end_date,period, baseline_window, field1, field2, field3, an_item1, an_item2, an_item3, L1KPI, L1Num, L1Den, num_function_1, den_function_1, L2KPI1, L2Num1, L2Den1, sub_num_function_1, sub_den_function_1, L2KPI2, L2Num2, L2Den2, sub_num_function_2, sub_den_function_2, L2KPI3, L2Num3, L2Den3, sub_num_function_3, sub_den_function_3, L2KPI4, L2Num4, L2Den4, sub_num_function_4, sub_den_function_4, an_date, kpiTrend, l1_items, ranking_field, ranking_function, bins_cols, bins_size, date_field)\n\n    elif((level1 !=\"\")   & (level2!=\"\") & (level3==\"\")):\n\n      l21 = L21_causality(df, file_path, level1,level2,level3,kpi1,start_date,end_date,period, baseline_window, field1, field2, field3, an_item1, an_item2, an_item3, L1KPI, L1Num, L1Den, num_function_1, den_function_1, L2KPI1, L2Num1, L2Den1, sub_num_function_1, sub_den_function_1, L2KPI2, L2Num2, L2Den2, sub_num_function_2, sub_den_function_2, L2KPI3, L2Num3, L2Den3, sub_num_function_3, sub_den_function_3, L2KPI4, L2Num4, L2Den4, sub_num_function_4, sub_den_function_4, an_date, kpiTrend, l1_items, ranking_field, ranking_function, bins_cols, bins_size, date_field)\n\n      l22 = L22_causality(df, file_path, level1,level2,level3,kpi1,start_date,end_date,period, baseline_window, field1, field2, field3, an_item1, an_item2, an_item3, L1KPI, L1Num, L1Den, num_function_1, den_function_1, L2KPI1, L2Num1, L2Den1, sub_num_function_1, sub_den_function_1, L2KPI2, L2Num2, L2Den2, sub_num_function_2, sub_den_function_2, L2KPI3, L2Num3, L2Den3, sub_num_function_3, sub_den_function_3, L2KPI4, L2Num4, L2Den4, sub_num_function_4, sub_den_function_4, an_date, kpiTrend, l1_items, l2_items, kpiTrend1, ranking_field, ranking_function, bins_cols, bins_size, date_field)\n\n    if((level1 !=\"\")   & ((level2 != '') & (l1_items != '')) & ((level3 != '') & (l2_items != ''))):\n\n      l21 = L21_causality(df, file_path, level1,level2,level3,kpi1,start_date,end_date,period, baseline_window, field1, field2, field3, an_item1, an_item2, an_item3, L1KPI, L1Num, L1Den, num_function_1, den_function_1, L2KPI1, L2Num1, L2Den1, sub_num_function_1, sub_den_function_1, L2KPI2, L2Num2, L2Den2, sub_num_function_2, sub_den_function_2, L2KPI3, L2Num3, L2Den3, sub_num_function_3, sub_den_function_3, L2KPI4, L2Num4, L2Den4, sub_num_function_4, sub_den_function_4, an_date, kpiTrend, l1_items, ranking_field, ranking_function, bins_cols, bins_size, date_field)\n\n      l22 = L22_causality(df, file_path, level1,level2,level3,kpi1,start_date,end_date,period, baseline_window, field1, field2, field3, an_item1, an_item2, an_item3, L1KPI, L1Num, L1Den, num_function_1, den_function_1, L2KPI1, L2Num1, L2Den1, sub_num_function_1, sub_den_function_1, L2KPI2, L2Num2, L2Den2, sub_num_function_2, sub_den_function_2, L2KPI3, L2Num3, L2Den3, sub_num_function_3, sub_den_function_3, L2KPI4, L2Num4, L2Den4, sub_num_function_4, sub_den_function_4, an_date, kpiTrend, l1_items, l2_items, kpiTrend1, ranking_field, ranking_function, bins_cols, bins_size, date_field)\n\n      l23 = L23_causality(df, file_path, level1,level2,level3,kpi1,start_date,end_date,period, baseline_window, field1, field2, field3, an_item1, an_item2, an_item3, L1KPI, L1Num, L1Den, num_function_1, den_function_1, L2KPI1, L2Num1, L2Den1, sub_num_function_1, sub_den_function_1, L2KPI2, L2Num2, L2Den2, sub_num_function_2, sub_den_function_2, L2KPI3, L2Num3, L2Den3, sub_num_function_3, sub_den_function_3, L2KPI4, L2Num4, L2Den4, sub_num_function_4, sub_den_function_4, an_date, kpiTrend, l1_items, l2_items, kpiTrend2, ranking_field, ranking_function, bins_cols, bins_size, date_field)\n      \n      \n  elif((L2KPI1 ==\"\")  & (L2KPI2==\"\") & (L2KPI3==\"\") & (L2KPI4 == \"\")):\n    \n    if((level1 !=\"\")   & (level2==\"\") & (level3==\"\")):\n      L21_causality_1KPI(df, file_path, level1,level2,level3,kpi1,start_date,end_date,period, baseline_window, field1, field2, field3, an_item1, an_item2, an_item3, L1KPI, L1Num, L1Den, num_function_1, den_function_1, an_date, kpiTrend, l1_items, ranking_field, ranking_function, bins_cols, bins_size, date_field)\n    \n    elif((level1 !=\"\")   & (level2!=\"\") & (level3==\"\")):\n    \n      L21_causality_1KPI(df, file_path, level1,level2,level3,kpi1,start_date,end_date,period, baseline_window, field1, field2, field3, an_item1, an_item2, an_item3, L1KPI, L1Num, L1Den, num_function_1, den_function_1, an_date, kpiTrend, l1_items, ranking_field, ranking_function, bins_cols, bins_size, date_field)\n\n      L22_causality_1KPI(df, file_path, level1,level2,level3,kpi1,start_date,end_date,period, baseline_window, field1, field2, field3, an_item1, an_item2, an_item3, L1KPI, L1Num, L1Den, num_function_1, den_function_1, an_date, kpiTrend, l1_items, l2_items,  ranking_field, ranking_function, bins_cols, bins_size, date_field)\n      \n    if((level1 !=\"\")   & (level2 !=\"\") & (level3 !=\"\")):\n      \n      L21_causality_1KPI(df, file_path, level1,level2,level3,kpi1,start_date,end_date,period, baseline_window, field1, field2, field3, an_item1, an_item2, an_item3, L1KPI, L1Num, L1Den, num_function_1, den_function_1, an_date, kpiTrend, l1_items, ranking_field, ranking_function, bins_cols, bins_size, date_field)\n\n      L22_causality_1KPI(df, file_path, level1,level2,level3,kpi1,start_date,end_date,period, baseline_window, field1, field2, field3, an_item1, an_item2, an_item3, L1KPI, L1Num, L1Den, num_function_1, den_function_1, an_date, kpiTrend, l1_items, l2_items, ranking_field, ranking_function, bins_cols, bins_size, date_field)\n      \n      L23_causality_1KPI(df, file_path, level1,level2,level3,kpi1,start_date,end_date,period, baseline_window, field1, field2, field3, an_item1, an_item2, an_item3, L1KPI, L1Num, L1Den, num_function_1, den_function_1, an_date, kpiTrend, l1_items, l2_items, ranking_field, ranking_function, bins_cols, bins_size, date_field)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"fc211be9-a9f4-4670-ac02-df198873272e"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["Preview snapshot and index numbers for anomalies and select one to execute causality analysis."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"877625c6-8ab8-4573-9b9c-8c228968ba31"}}},{"cell_type":"code","source":["import pandas as pd\ndf = pd.read_csv('/dbfs/FileStore/CG/output_chubb/data2.csv')\nanomaly_indexes = pd.read_csv('/dbfs/FileStore/CG/output_chubb/anomalies2.csv')\nanomaly_indexes = anomaly_indexes.loc[anomaly_indexes['anomaly_score'] == 1]\ndisplay(anomaly_indexes)\n#display(df)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d29b02a1-b162-4cca-ad05-4dd856b6dbe6"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"overflow":false,"datasetInfos":[],"data":[[2019,7,"MAYBANK","ACCIDENTAL MEDICAL",8212.0,24142.339147712963,13,260.3583569999988,2716,0.3401493098806846,631.6923076923077,0.049931180046585,8.8889319395114,0.0958609561855665,"2019-07-01","2019-07-01","MAYBANK,ACCIDENTAL MEDICAL",0.0307358992315034,0.0289409178178612,1.0,698.7014532048942,6.0],[2020,7,"MAYBANK","ACCIDENTAL MEDICAL",13520.226899999998,30127.66638550269,20,300.7604019999975,2999,0.4487644919789033,676.0113449999999,0.0664981156661712,10.045904096533071,0.1002868962987654,"2020-07-01","2020-07-01","MAYBANK,ACCIDENTAL MEDICAL",0.0367905735814495,0.0177129558430466,1.0,533.6500243404503,5.0],[2017,10,"MAYBANK","ACCIDENTAL MEDICAL",1912.61,10492.513062310509,4,119.84257700000032,1429,0.1822833089310286,478.1525,0.0333771193855418,7.342556376704345,0.0838646445066482,"2017-10-01","2017-10-01","MAYBANK,ACCIDENTAL MEDICAL",0.0294787488646375,0.0294787488646375,1.0,309.3061575227809,4.0],[2021,7,"MAYBANK","ACCIDENTAL MEDICAL",3412.0657,36843.972949311246,6,339.93307099999544,3384,0.0926085171296323,568.6776166666667,0.0176505333310158,10.887698862089612,0.1004530351654832,"2021-07-01","2021-07-01","MAYBANK,ACCIDENTAL MEDICAL",-0.0146994799040206,0.007048682817398,1.0,259.70147905248984,3.0],[2019,4,"MAYBANK","ACCIDENTAL MEDICAL",7331.69,22056.427704840185,17,248.4780200000006,2593,0.332406049525014,431.2758823529412,0.0684165142655272,8.506142578033238,0.0958264635557271,"2019-04-01","2019-04-01","MAYBANK,ACCIDENTAL MEDICAL",0.0017949814136422,0.0104908242554575,1.0,231.39010675468333,2.0],[2021,10,"MAYBANK","ACCIDENTAL MEDICAL",5152.836,12328.54459394274,3,114.43814499999984,2088,0.4179597973415038,1717.612,0.0262150352052631,5.904475380240776,0.0548075407088121,"2021-10-01","2021-10-01","MAYBANK,ACCIDENTAL MEDICAL",-0.0026826167379597,0.0120168631660609,1.0,148.15043342208983,1.0]],"plotOptions":{"displayType":"table","customPlotOptions":{},"pivotColumns":null,"pivotAggregation":null,"xColumns":null,"yColumns":null},"columnCustomDisplayInfos":{},"aggType":"","isJsonSchema":true,"removedWidgets":[],"aggSchema":[],"schema":[{"name":"Accd_year","type":"\"long\"","metadata":"{}"},{"name":"Accd_quarter","type":"\"long\"","metadata":"{}"},{"name":"SPONSOR_NAME","type":"\"string\"","metadata":"{}"},{"name":"COVERAGE_GROUP","type":"\"string\"","metadata":"{}"},{"name":"ULT_CLAIM_INCURRED","type":"\"double\"","metadata":"{}"},{"name":"ULT_EARNED_PREMIUM","type":"\"double\"","metadata":"{}"},{"name":"CLAIMS_COUNT","type":"\"long\"","metadata":"{}"},{"name":"AVG_EARNED_EXP_POLCOVG","type":"\"double\"","metadata":"{}"},{"name":"POL_CVG_COUNT","type":"\"long\"","metadata":"{}"},{"name":"ALR","type":"\"double\"","metadata":"{}"},{"name":"CLAIMS_SEVERITY","type":"\"double\"","metadata":"{}"},{"name":"CLAIMS_FREQUENCY","type":"\"double\"","metadata":"{}"},{"name":"AVG_PREMIUM","type":"\"double\"","metadata":"{}"},{"name":"EXPOSURE","type":"\"double\"","metadata":"{}"},{"name":"quart_year","type":"\"string\"","metadata":"{}"},{"name":"timestamp","type":"\"string\"","metadata":"{}"},{"name":"combo","type":"\"string\"","metadata":"{}"},{"name":"Trend","type":"\"double\"","metadata":"{}"},{"name":"Change","type":"\"double\"","metadata":"{}"},{"name":"anomaly_score","type":"\"double\"","metadata":"{}"},{"name":"Premium_Score","type":"\"double\"","metadata":"{}"},{"name":"final_rank","type":"\"double\"","metadata":"{}"}],"aggError":"","aggData":[],"addedWidgets":{},"metadata":{},"dbfsResultPath":null,"type":"table","aggOverflow":false,"aggSeriesLimitReached":false,"arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>Accd_year</th><th>Accd_quarter</th><th>SPONSOR_NAME</th><th>COVERAGE_GROUP</th><th>ULT_CLAIM_INCURRED</th><th>ULT_EARNED_PREMIUM</th><th>CLAIMS_COUNT</th><th>AVG_EARNED_EXP_POLCOVG</th><th>POL_CVG_COUNT</th><th>ALR</th><th>CLAIMS_SEVERITY</th><th>CLAIMS_FREQUENCY</th><th>AVG_PREMIUM</th><th>EXPOSURE</th><th>quart_year</th><th>timestamp</th><th>combo</th><th>Trend</th><th>Change</th><th>anomaly_score</th><th>Premium_Score</th><th>final_rank</th></tr></thead><tbody><tr><td>2019</td><td>7</td><td>MAYBANK</td><td>ACCIDENTAL MEDICAL</td><td>8212.0</td><td>24142.339147712963</td><td>13</td><td>260.3583569999988</td><td>2716</td><td>0.3401493098806846</td><td>631.6923076923077</td><td>0.049931180046585</td><td>8.8889319395114</td><td>0.0958609561855665</td><td>2019-07-01</td><td>2019-07-01</td><td>MAYBANK,ACCIDENTAL MEDICAL</td><td>0.0307358992315034</td><td>0.0289409178178612</td><td>1.0</td><td>698.7014532048942</td><td>6.0</td></tr><tr><td>2020</td><td>7</td><td>MAYBANK</td><td>ACCIDENTAL MEDICAL</td><td>13520.226899999998</td><td>30127.66638550269</td><td>20</td><td>300.7604019999975</td><td>2999</td><td>0.4487644919789033</td><td>676.0113449999999</td><td>0.0664981156661712</td><td>10.045904096533071</td><td>0.1002868962987654</td><td>2020-07-01</td><td>2020-07-01</td><td>MAYBANK,ACCIDENTAL MEDICAL</td><td>0.0367905735814495</td><td>0.0177129558430466</td><td>1.0</td><td>533.6500243404503</td><td>5.0</td></tr><tr><td>2017</td><td>10</td><td>MAYBANK</td><td>ACCIDENTAL MEDICAL</td><td>1912.61</td><td>10492.513062310509</td><td>4</td><td>119.84257700000032</td><td>1429</td><td>0.1822833089310286</td><td>478.1525</td><td>0.0333771193855418</td><td>7.342556376704345</td><td>0.0838646445066482</td><td>2017-10-01</td><td>2017-10-01</td><td>MAYBANK,ACCIDENTAL MEDICAL</td><td>0.0294787488646375</td><td>0.0294787488646375</td><td>1.0</td><td>309.3061575227809</td><td>4.0</td></tr><tr><td>2021</td><td>7</td><td>MAYBANK</td><td>ACCIDENTAL MEDICAL</td><td>3412.0657</td><td>36843.972949311246</td><td>6</td><td>339.93307099999544</td><td>3384</td><td>0.0926085171296323</td><td>568.6776166666667</td><td>0.0176505333310158</td><td>10.887698862089612</td><td>0.1004530351654832</td><td>2021-07-01</td><td>2021-07-01</td><td>MAYBANK,ACCIDENTAL MEDICAL</td><td>-0.0146994799040206</td><td>0.007048682817398</td><td>1.0</td><td>259.70147905248984</td><td>3.0</td></tr><tr><td>2019</td><td>4</td><td>MAYBANK</td><td>ACCIDENTAL MEDICAL</td><td>7331.69</td><td>22056.427704840185</td><td>17</td><td>248.4780200000006</td><td>2593</td><td>0.332406049525014</td><td>431.2758823529412</td><td>0.0684165142655272</td><td>8.506142578033238</td><td>0.0958264635557271</td><td>2019-04-01</td><td>2019-04-01</td><td>MAYBANK,ACCIDENTAL MEDICAL</td><td>0.0017949814136422</td><td>0.0104908242554575</td><td>1.0</td><td>231.39010675468333</td><td>2.0</td></tr><tr><td>2021</td><td>10</td><td>MAYBANK</td><td>ACCIDENTAL MEDICAL</td><td>5152.836</td><td>12328.54459394274</td><td>3</td><td>114.43814499999984</td><td>2088</td><td>0.4179597973415038</td><td>1717.612</td><td>0.0262150352052631</td><td>5.904475380240776</td><td>0.0548075407088121</td><td>2021-10-01</td><td>2021-10-01</td><td>MAYBANK,ACCIDENTAL MEDICAL</td><td>-0.0026826167379597</td><td>0.0120168631660609</td><td>1.0</td><td>148.15043342208983</td><td>1.0</td></tr></tbody></table></div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["Run Causality Analysis"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c531be1f-0964-48be-94e7-584efc949272"}}},{"cell_type":"markdown","source":["- I have created a separate Anomaly Detection notebook within the UAT folder named Anomaly_Detection_v3.1_Trend_Causality with new file paths so as not to interfere with testing of other notebooks. The new Anomaly Detection notebook must be used in order for results to be transfered to this one.\n\n- For best results, use either the Sens Slope or Beta Slope trend detection algorithms for anomaly inputs in this notebook. Anomalies detected with point algorithms will still run, but they may or may not have any meaningful slope and results may be less significant. \n\n- Pass parameters to invoke the main function and fetch results. (In subsequent iterations these parameters will be passed via the user interface)\n\n- Please note that the required libraries will be installed at each run of this notebook (handled in the code) since libraries are not yet available at cluster level.\n\n- baseline_window dictates the number of periods considered in the trend calculations\n\n- The first table output beneath Command 11 where the main function is called will display a list of all the anomalies from the proceeding anomaly detection run. From this, the user must input the date of the anomaly they wish to perform causal analysis on in the variable an_date. They must also provide an input coinciding with the level of aggregation from the anomaly detection run for the an_item1 variable. eg. if anomaly detection was run on the coverage level, the user must provide the specific coverage from the chosen anomaly to perform causal analysis on.\n\n- level1, level2, level3, define the drill down levels. l1_items specifies the specific value of the level the user wishes to drill down into. For example, after drilling down on level1 to SPONSOR_NAME, l1_items should reflect the specific sponsor desired for the next drill down eg. 'AMERICAN EXPRESS'.\n\n- field1,field2, field3 (MUST mirror the aggregation level used in the proceeding anomaly detection run): If anomaly detection was aggregated at the monthly coverage level, the setup here must reflect that for the causality to function properly.\n\n- start_date and end_date : Filter data on selected dates. eg: start_date =\"'2018-01-01'\", end_date = \"'2020-12-31'\"\n\n- period = option : 'month' and 'quarter'\n\n- L1KPI, L2KPI1, L2KPI2, L2KPI3, each have their own corresponding numerators (eg.L1Num) and denominators (eg.L1Den) which are used to calculate the KPI's in a user defined manner."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e76c75d8-0f36-45bd-b1aa-550319ca868e"}}},{"cell_type":"code","source":["#main(file_path ='/dbfs/sg_claims_files/', baseline_window = 8, L1KPI = 'ALR', L1Num = 'ULT_CLAIM_INCURRED', L1Den = 'ULT_EARNED_PREMIUM', num_function_1 = 'sum', den_function_1= 'sum', L2KPI1 = 'Severity', L2Num1 = 'ULT_CLAIM_INCURRED', L2Den1 = 'CLAIMS_COUNT', sub_num_function_1 = 'sum', sub_den_function_1= 'nunique', L2KPI2 = 'Frequency', L2Num2 = 'CLAIMS_COUNT', L2Den2 = 'AVG_EARNED_EXP_POLCOVG', sub_num_function_2 = 'nunique', sub_den_function_2= 'sum', L2KPI3 = 'Average_Premium', L2Num3 = 'ULT_EARNED_PREMIUM', L2Den3 = 'POL_CVG_COUNT', sub_num_function_3 = 'sum', sub_den_function_3= 'nunique', L2KPI4 = 'EXPOSURE', L2Num4 = 'AVG_EARNED_EXP_POLCOVG', L2Den4= 'POL_CVG_COUNT', sub_num_function_4 = 'sum', sub_den_function_4= 'nunique',ranking_field ='ULT_EARNED_PREMIUM', ranking_function = 'sum', kpi1 = 'ALR',  level1 = 'COVERAGE_GROUP', level2 = 'PROD_CLASS_DESC', level3 = 'GENDER', field1 = 'SPONSOR_NAME', field2 = '', field3 = '', start_date = \"'2016-01-01'\", end_date = \"'2021-12-31'\", period = 'quarter', an_date = '2019-01-01', an_item1 = 'MAYBANK', an_item2 = '', an_item3 = '', l1_items = 'IN HOSPITAL CASH', l2_items = 'P/A', bins_cols=['AGE'],bins_size=[3], date_field = 'ACCD_DATE')\n\n#main(file_path ='/dbfs/sg_claims_files/', baseline_window = 8, L1KPI = 'ALR', L1Num = 'ULT_CLAIM_INCURRED', L1Den = 'ULT_EARNED_PREMIUM', num_function_1 = 'sum', den_function_1= 'sum', L2KPI1 = '', L2Num1 = '', L2Den1 = '', sub_num_function_1 = '', sub_den_function_1= '', L2KPI2 = '', L2Num2 = '', L2Den2 = '', sub_num_function_2 = '', sub_den_function_2= '', L2KPI3 = '', L2Num3 = '', L2Den3 = '', sub_num_function_3 = '', sub_den_function_3= '', L2KPI4 = '', L2Num4 = '', L2Den4= '', sub_num_function_4 = '', sub_den_function_4= '',ranking_field ='ULT_EARNED_PREMIUM', ranking_function = 'sum', kpi1 = 'ALR',  level1 = 'COVERAGE_GROUP', level2 = 'PROD_CLASS_DESC', level3 = 'GENDER', field1 = 'SPONSOR_NAME', field2 = '', field3 = '', start_date = \"'2016-01-01'\", end_date = \"'2021-12-31'\", period = 'quarter', an_date = '2019-01-01', an_item1 = 'MAYBANK', an_item2 = '', an_item3 = '', l1_items = 'IN HOSPITAL CASH', l2_items = 'P/A', bins_cols=['AGE'],bins_size=[3], date_field = 'ACCD_DATE')\n\n#Two Filter AD Run\n\nmain(file_path ='/dbfs/sg_claims_files/', baseline_window = 8, L1KPI = 'ALR', L1Num = 'ULT_CLAIM_INCURRED', L1Den = 'ULT_EARNED_PREMIUM', num_function_1 = 'sum', den_function_1= 'sum', L2KPI1 = 'Severity', L2Num1 = 'ULT_CLAIM_INCURRED', L2Den1 = 'CLAIMS_COUNT', sub_num_function_1 = 'sum', sub_den_function_1= 'nunique', L2KPI2 = 'Frequency', L2Num2 = 'CLAIMS_COUNT', L2Den2 = 'AVG_EARNED_EXP_POLCOVG', sub_num_function_2 = 'nunique', sub_den_function_2= 'sum', L2KPI3 = 'Average_Premium', L2Num3 = 'ULT_EARNED_PREMIUM', L2Den3 = 'POL_CVG_COUNT', sub_num_function_3 = 'sum', sub_den_function_3= 'nunique', L2KPI4 = 'EXPOSURE', L2Num4 = 'AVG_EARNED_EXP_POLCOVG', L2Den4= 'POL_CVG_COUNT', sub_num_function_4 = 'sum', sub_den_function_4= 'nunique',ranking_field ='ULT_EARNED_PREMIUM', ranking_function = 'sum', kpi1 = 'Frequency',  level1 = 'GENDER', level2 = '', level3 = '', field1 = 'SPONSOR_NAME', field2 = 'COVERAGE_GROUP', field3 = '', start_date = \"'2016-01-01'\", end_date = \"'2021-12-31'\", period = 'quarter', an_date = '2020-07-01', an_item1 = 'MAYBANK', an_item2 = 'ACCIDENTAL MEDICAL', an_item3 = '', l1_items = 'Female', l2_items = '', bins_cols=['AGE'],bins_size=[3], date_field = 'ACCD_DATE')\n\n#singapore billings database\n\n#main(file_path ='/dbfs/Discovery/SG_BILLING_DATA/SG_Billing.csv', baseline_window = 8, L1KPI = 'PAID RATIO', L1Num = 'Paid_charges', L1Den = 'charge', num_function_1 = 'sum', den_function_1= 'sum', L2KPI1 = '', L2Num1 = '', L2Den1 = '', sub_num_function_1 = '', sub_den_function_1= '', L2KPI2 = '', L2Num2 = '', L2Den2 = '', sub_num_function_2 = '', sub_den_function_2= '', L2KPI3 = '', L2Num3 = '', L2Den3 = '', sub_num_function_3 = '', sub_den_function_3= '', L2KPI4 = '', L2Num4 = '', L2Den4= '', sub_num_function_4 = '', sub_den_function_4= '',ranking_field ='bill_AGE', ranking_function = 'sum', kpi1 = 'PAID RATIO',  level1 = 'PROD_NAME', level2 = 'CHANNEL', level3 = 'BILL_METHOD', field1 = 'SPONSOR_NAME', field2 = '', field3 = '', start_date = \"'2016-01-01'\", end_date = \"'2021-12-31'\", period = 'quarter', an_date = '2021-07-01', an_item1 = 'American Express', an_item2 = '', an_item3 = '', l1_items = 'True Vital Recovery', l2_items = 'Telemarketing', bins_cols=['Customer_Age'],bins_size=[3], date_field = 'BILL_DUE_DATE')"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a5b0caa0-40e5-4974-823a-cbe0424246b4"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">0.0367905735814495\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">0.0367905735814495\n</div>"]}},{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"overflow":false,"datasetInfos":[],"data":[["2020-07-01","MAYBANK",0.0034229530021003983,-0.006035395986229244,0.03138963178437526,0.01578774216710147,-0.04635869566431117,0.24110802178131827,0.12126779021976662,-0.1640473468799734,0.8531976734443314,0.42912465423104773,1.1182749807954058]],"plotOptions":{"displayType":"table","customPlotOptions":{},"pivotColumns":null,"pivotAggregation":null,"xColumns":null,"yColumns":null},"columnCustomDisplayInfos":{},"aggType":"","isJsonSchema":true,"removedWidgets":[],"aggSchema":[],"schema":[{"name":"timestamp","type":"\"string\"","metadata":"{}"},{"name":"SPONSOR_NAME","type":"\"string\"","metadata":"{}"},{"name":"Frequency_Trend","type":"\"double\"","metadata":"{}"},{"name":"Average_Premium_cont","type":"\"double\"","metadata":"{}"},{"name":"Severity_cont","type":"\"double\"","metadata":"{}"},{"name":"Frequency_cont","type":"\"double\"","metadata":"{}"},{"name":"Average_Premium_%ALR","type":"\"double\"","metadata":"{}"},{"name":"Severity_%ALR","type":"\"double\"","metadata":"{}"},{"name":"Frequency_%ALR","type":"\"double\"","metadata":"{}"},{"name":"Average_Premium_atr","type":"\"double\"","metadata":"{}"},{"name":"Severity_atr","type":"\"double\"","metadata":"{}"},{"name":"Frequency_atr","type":"\"double\"","metadata":"{}"},{"name":"ALR_atr","type":"\"double\"","metadata":"{}"}],"aggError":"","aggData":[],"addedWidgets":{},"metadata":{},"dbfsResultPath":null,"type":"table","aggOverflow":false,"aggSeriesLimitReached":false,"arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>timestamp</th><th>SPONSOR_NAME</th><th>Frequency_Trend</th><th>Average_Premium_cont</th><th>Severity_cont</th><th>Frequency_cont</th><th>Average_Premium_%ALR</th><th>Severity_%ALR</th><th>Frequency_%ALR</th><th>Average_Premium_atr</th><th>Severity_atr</th><th>Frequency_atr</th><th>ALR_atr</th></tr></thead><tbody><tr><td>2020-07-01</td><td>MAYBANK</td><td>0.0034229530021003983</td><td>-0.006035395986229244</td><td>0.03138963178437526</td><td>0.01578774216710147</td><td>-0.04635869566431117</td><td>0.24110802178131827</td><td>0.12126779021976662</td><td>-0.1640473468799734</td><td>0.8531976734443314</td><td>0.42912465423104773</td><td>1.1182749807954058</td></tr></tbody></table></div>"]}},{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">file read from dbfs started\nfile read from dbfs ended 23.2562313079834\n[&#39;2018-10-01&#39;, &#39;2019-01-01&#39;, &#39;2019-04-01&#39;, &#39;2019-07-01&#39;, &#39;2019-10-01&#39;, &#39;2020-01-01&#39;, &#39;2020-04-01&#39;, &#39;2020-07-01&#39;]\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">file read from dbfs started\nfile read from dbfs ended 23.2562313079834\n[&#39;2018-10-01&#39;, &#39;2019-01-01&#39;, &#39;2019-04-01&#39;, &#39;2019-07-01&#39;, &#39;2019-10-01&#39;, &#39;2020-01-01&#39;, &#39;2020-04-01&#39;, &#39;2020-07-01&#39;]\n</div>"]}},{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"overflow":false,"datasetInfos":[],"data":[["2020-07-01T00:00:00.000+0000","Male                          ",0.022374524801112156,0.0030026764496954906,0.045924974828715034,0.033121368398435734,0.0792708725253798,0.08161537473853075,0.9002677907456206,1.0],["2020-07-01T00:00:00.000+0000","Female                        ",0.07033421587558879,6.665287333182743E-4,0.045924974828715034,0.033121368398435734,0.009244231042524352,0.018116834515848664,0.9002677907456206,1.0]],"plotOptions":{"displayType":"table","customPlotOptions":{},"pivotColumns":null,"pivotAggregation":null,"xColumns":null,"yColumns":null},"columnCustomDisplayInfos":{},"aggType":"","isJsonSchema":true,"removedWidgets":[],"aggSchema":[],"schema":[{"name":"timestamp","type":"\"timestamp\"","metadata":"{}"},{"name":"GENDER","type":"\"string\"","metadata":"{}"},{"name":"ALR_Trend","type":"\"double\"","metadata":"{}"},{"name":"Frequency_Trend_Decomp","type":"\"double\"","metadata":"{}"},{"name":"Agg_ALR_Trend_Decomp","type":"\"double\"","metadata":"{}"},{"name":"Frequency_Business_Mix","type":"\"double\"","metadata":"{}"},{"name":"Frequency_Impact_%Frequency","type":"\"double\"","metadata":"{}"},{"name":"Frequency_Attribution","type":"\"double\"","metadata":"{}"},{"name":"Frequency_Att_Business_Mix","type":"\"double\"","metadata":"{}"},{"name":"Frequency_Att_Agg","type":"\"double\"","metadata":"{}"}],"aggError":"","aggData":[],"addedWidgets":{},"metadata":{},"dbfsResultPath":null,"type":"table","aggOverflow":false,"aggSeriesLimitReached":false,"arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>timestamp</th><th>GENDER</th><th>ALR_Trend</th><th>Frequency_Trend_Decomp</th><th>Agg_ALR_Trend_Decomp</th><th>Frequency_Business_Mix</th><th>Frequency_Impact_%Frequency</th><th>Frequency_Attribution</th><th>Frequency_Att_Business_Mix</th><th>Frequency_Att_Agg</th></tr></thead><tbody><tr><td>2020-07-01T00:00:00.000+0000</td><td>Male                          </td><td>0.022374524801112156</td><td>0.0030026764496954906</td><td>0.045924974828715034</td><td>0.033121368398435734</td><td>0.0792708725253798</td><td>0.08161537473853075</td><td>0.9002677907456206</td><td>1.0</td></tr><tr><td>2020-07-01T00:00:00.000+0000</td><td>Female                        </td><td>0.07033421587558879</td><td>6.665287333182743E-4</td><td>0.045924974828715034</td><td>0.033121368398435734</td><td>0.009244231042524352</td><td>0.018116834515848664</td><td>0.9002677907456206</td><td>1.0</td></tr></tbody></table></div>"]}}],"execution_count":0},{"cell_type":"code","source":[""],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"eb5acdf0-ed43-4f86-ad53-ccc5929faace"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":[""],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"be71708a-e2f9-49dc-b99e-7af5130997e0"}},"outputs":[],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"trend_causality_v1.4","dashboards":[],"notebookMetadata":{"pythonIndentUnit":2},"language":"python","widgets":{},"notebookOrigID":3750781121782634}},"nbformat":4,"nbformat_minor":0}
